"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2321],{1870:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"databricks-basic-data-engineering","title":"Databricks Data Engineering: Foundations and Core Concepts","description":"Introduction to the Databricks Lakehouse Platform","source":"@site/docs/databricks-basic-data-engineering.md","sourceDirName":".","slug":"/databricks-basic-data-engineering","permalink":"/docusaurus-tutorial/docs/databricks-basic-data-engineering","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/databricks-basic-data-engineering.md","tags":[],"version":"current","frontMatter":{}}');var s=a(4848),i=a(8453);const r={},o="Databricks Data Engineering: Foundations and Core Concepts",l={},d=[{value:"Introduction to the Databricks Lakehouse Platform",id:"introduction-to-the-databricks-lakehouse-platform",level:2},{value:"What is the Lakehouse Architecture?",id:"what-is-the-lakehouse-architecture",level:3},{value:"Databricks Workspace Components",id:"databricks-workspace-components",level:3},{value:"Day 1: Core Data Engineering Concepts",id:"day-1-core-data-engineering-concepts",level:2},{value:"1. Delta Lake",id:"1-delta-lake",level:3},{value:"Key Delta Lake Features",id:"key-delta-lake-features",level:4},{value:"Hands-on Exercise: Working with Delta Tables",id:"hands-on-exercise-working-with-delta-tables",level:4},{value:"2. Relational Entities on Databricks",id:"2-relational-entities-on-databricks",level:3},{value:"Databases and Tables",id:"databases-and-tables",level:4},{value:"Views and Temporary Views",id:"views-and-temporary-views",level:4},{value:"Hands-on Exercise: Creating a Data Model",id:"hands-on-exercise-creating-a-data-model",level:4},{value:"3. ETL with Spark SQL",id:"3-etl-with-spark-sql",level:3},{value:"Data Extraction",id:"data-extraction",level:4},{value:"Data Transformation",id:"data-transformation",level:4},{value:"Data Loading",id:"data-loading",level:4},{value:"Hands-on Exercise: Building an ETL Pipeline",id:"hands-on-exercise-building-an-etl-pipeline",level:4},{value:"4. Just Enough Python for Spark SQL",id:"4-just-enough-python-for-spark-sql",level:3},{value:"PySpark Basics",id:"pyspark-basics",level:4},{value:"Custom Functions with Python UDFs",id:"custom-functions-with-python-udfs",level:4},{value:"Combining SQL and Python",id:"combining-sql-and-python",level:4},{value:"Hands-on Exercise: Extending SQL with Python",id:"hands-on-exercise-extending-sql-with-python",level:4},{value:"5. Incremental Data Processing with Structured Streaming and Auto Loader",id:"5-incremental-data-processing-with-structured-streaming-and-auto-loader",level:3},{value:"Structured Streaming",id:"structured-streaming",level:4},{value:"Auto Loader",id:"auto-loader",level:4},{value:"Hands-on Exercise: Implementing Incremental Processing",id:"hands-on-exercise-implementing-incremental-processing",level:4},{value:"Day 2: Advanced Data Engineering",id:"day-2-advanced-data-engineering",level:2},{value:"1. Medallion Architecture in the Data Lakehouse",id:"1-medallion-architecture-in-the-data-lakehouse",level:3},{value:"Bronze Layer",id:"bronze-layer",level:4},{value:"Silver Layer",id:"silver-layer",level:4},{value:"Gold Layer",id:"gold-layer",level:4},{value:"Hands-on Exercise: Implementing a Medallion Architecture",id:"hands-on-exercise-implementing-a-medallion-architecture",level:4},{value:"2. Delta Live Tables",id:"2-delta-live-tables",level:3},{value:"Python API for DLT",id:"python-api-for-dlt",level:4},{value:"SQL API for DLT",id:"sql-api-for-dlt",level:4},{value:"Hands-on Exercise: Building a DLT Pipeline",id:"hands-on-exercise-building-a-dlt-pipeline",level:4},{value:"3. Task Orchestration with Databricks Jobs",id:"3-task-orchestration-with-databricks-jobs",level:3},{value:"Creating and Scheduling Jobs",id:"creating-and-scheduling-jobs",level:4},{value:"Job Parameters and Dynamic Values",id:"job-parameters-and-dynamic-values",level:4},{value:"Error Handling and Notifications",id:"error-handling-and-notifications",level:4},{value:"Hands-on Exercise: Orchestrating Workflows",id:"hands-on-exercise-orchestrating-workflows",level:4},{value:"4. Databricks SQL",id:"4-databricks-sql",level:3},{value:"Creating and Optimizing SQL Warehouses",id:"creating-and-optimizing-sql-warehouses",level:4},{value:"Writing Efficient Queries",id:"writing-efficient-queries",level:4},{value:"Building Dashboards",id:"building-dashboards",level:4},{value:"Hands-on Exercise: Analytics with Databricks SQL",id:"hands-on-exercise-analytics-with-databricks-sql",level:4},{value:"5. Managing Permissions in the Lakehouse",id:"5-managing-permissions-in-the-lakehouse",level:3},{value:"Unity Catalog",id:"unity-catalog",level:4},{value:"Table Access Control",id:"table-access-control",level:4},{value:"Dynamic Views for Row-Level Security",id:"dynamic-views-for-row-level-security",level:4},{value:"Hands-on Exercise: Implementing Security",id:"hands-on-exercise-implementing-security",level:4},{value:"6. Productionizing Dashboards and Queries on Databricks SQL",id:"6-productionizing-dashboards-and-queries-on-databricks-sql",level:3},{value:"Query Optimization Techniques",id:"query-optimization-techniques",level:4},{value:"Alerts and Scheduled Queries",id:"alerts-and-scheduled-queries",level:4},{value:"Dashboard Parameters and Interactivity",id:"dashboard-parameters-and-interactivity",level:4},{value:"Hands-on Exercise: Production-Ready Analytics",id:"hands-on-exercise-production-ready-analytics",level:4},{value:"Summary and Additional Resources",id:"summary-and-additional-resources",level:2},{value:"Learning Resources",id:"learning-resources",level:3},{value:"Next Steps",id:"next-steps",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"databricks-data-engineering-foundations-and-core-concepts",children:"Databricks Data Engineering: Foundations and Core Concepts"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-the-databricks-lakehouse-platform",children:"Introduction to the Databricks Lakehouse Platform"}),"\n",(0,s.jsx)(n.p,{children:"The Databricks Lakehouse Platform combines the best features of data lakes and data warehouses to provide a unified solution for data engineering, analytics, and machine learning. This training material covers essential concepts and practical techniques for developing data pipelines on this platform."}),"\n",(0,s.jsx)(n.h3,{id:"what-is-the-lakehouse-architecture",children:"What is the Lakehouse Architecture?"}),"\n",(0,s.jsx)(n.p,{children:"The Lakehouse architecture addresses the limitations of traditional data lakes and warehouses by offering:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ACID transactions"})," - Ensuring data consistency and reliability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema enforcement and governance"})," - Maintaining data quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BI support"})," - Enabling direct analytics on the data lake"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoupled storage and compute"})," - Providing cost efficiency and scalability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open formats"})," - Preventing vendor lock-in with formats like Delta Lake"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"databricks-workspace-components",children:"Databricks Workspace Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Notebooks"})," - Collaborative environment for code development"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clusters"})," - Scalable compute resources for processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jobs"})," - Scheduled execution of workflows"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Repos"})," - Git-based version control integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SQL Warehouses"})," - Dedicated compute for SQL analytics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unity Catalog"})," - Unified governance layer for data assets"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"day-1-core-data-engineering-concepts",children:"Day 1: Core Data Engineering Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"1-delta-lake",children:"1. Delta Lake"}),"\n",(0,s.jsx)(n.p,{children:"Delta Lake is an open-source storage layer that brings reliability to data lakes. It provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing."}),"\n",(0,s.jsx)(n.h4,{id:"key-delta-lake-features",children:"Key Delta Lake Features"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Creating Delta Tables:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Creating a managed Delta table\nCREATE TABLE customer_data (\n  id INT,\n  name STRING,\n  email STRING,\n  signup_date DATE\n)\n\n-- Creating an external Delta table\nCREATE TABLE transactions\nLOCATION '/mnt/data/transactions'\nAS SELECT * FROM parquet.`/mnt/raw-data/transactions`\n\n-- Converting an existing table to Delta\nCONVERT TO DELTA parquet.`/mnt/data/events`\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Time Travel:"})}),"\n",(0,s.jsx)(n.p,{children:"Delta Lake maintains a history of changes, allowing you to query data as it appeared at a specific point in time."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Query data as of a timestamp\nSELECT * FROM customer_data TIMESTAMP AS OF '2023-01-01'\n\n-- Query data as of a version number\nSELECT * FROM customer_data VERSION AS OF 5\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Optimization Commands:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Compact small files\nOPTIMIZE customer_data\n\n-- Z-order optimization for query performance\nOPTIMIZE customer_data ZORDER BY (id, signup_date)\n\n-- Vacuum old files (default retention: 7 days)\nVACUUM customer_data\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-working-with-delta-tables",children:"Hands-on Exercise: Working with Delta Tables"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create a Delta table from sample data"}),"\n",(0,s.jsx)(n.li,{children:"Perform updates and deletes on the table"}),"\n",(0,s.jsx)(n.li,{children:"View the table history"}),"\n",(0,s.jsx)(n.li,{children:"Query previous versions"}),"\n",(0,s.jsx)(n.li,{children:"Optimize the table for better performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-relational-entities-on-databricks",children:"2. Relational Entities on Databricks"}),"\n",(0,s.jsx)(n.p,{children:"Databricks organizes data using familiar relational database concepts while leveraging the scalable cloud storage underneath."}),"\n",(0,s.jsx)(n.h4,{id:"databases-and-tables",children:"Databases and Tables"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a database\nCREATE DATABASE retail_analytics\nLOCATION '/mnt/data/retail'\n\n-- Create a table with partitioning\nCREATE TABLE retail_analytics.sales (\n  transaction_id STRING,\n  product_id STRING,\n  customer_id STRING,\n  store_id STRING,\n  quantity INT,\n  price DOUBLE,\n  transaction_date DATE\n)\nPARTITIONED BY (transaction_date)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"views-and-temporary-views",children:"Views and Temporary Views"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a view\nCREATE VIEW retail_analytics.daily_sales AS\nSELECT transaction_date, SUM(quantity * price) AS daily_revenue\nFROM retail_analytics.sales\nGROUP BY transaction_date\n\n-- Create a temporary view (session-scoped)\nCREATE TEMP VIEW recent_sales AS\nSELECT * FROM retail_analytics.sales\nWHERE transaction_date >= current_date() - INTERVAL 30 DAYS\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-creating-a-data-model",children:"Hands-on Exercise: Creating a Data Model"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Design a star schema for a retail analytics use case"}),"\n",(0,s.jsx)(n.li,{children:"Create dimension and fact tables"}),"\n",(0,s.jsx)(n.li,{children:"Implement appropriate partitioning strategies"}),"\n",(0,s.jsx)(n.li,{children:"Create views for common access patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-etl-with-spark-sql",children:"3. ETL with Spark SQL"}),"\n",(0,s.jsx)(n.p,{children:"Spark SQL is a powerful tool for data transformation within the Databricks environment, offering both performance and ease of use."}),"\n",(0,s.jsx)(n.h4,{id:"data-extraction",children:"Data Extraction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Reading from cloud storage (CSV)\nCREATE TABLE raw_customer_data\nUSING CSV\nOPTIONS (\n  path = '/mnt/raw/customers.csv',\n  header = 'true',\n  inferSchema = 'true'\n)\n\n-- Reading from JDBC source\nCREATE TABLE raw_orders\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url = 'jdbc:postgresql://server:5432/database',\n  dbtable = 'orders',\n  user = '${user}',\n  password = '${password}'\n)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"data-transformation",children:"Data Transformation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Basic transformations\nCREATE TABLE transformed_customers AS\nSELECT \n  id,\n  UPPER(first_name) AS first_name,\n  UPPER(last_name) AS last_name,\n  email,\n  CASE\n    WHEN age < 18 THEN 'Under 18'\n    WHEN age BETWEEN 18 AND 35 THEN 'Young Adult'\n    WHEN age BETWEEN 36 AND 65 THEN 'Adult'\n    ELSE 'Senior'\n  END AS age_group\nFROM raw_customer_data\n\n-- Window functions\nCREATE TABLE customer_purchase_stats AS\nSELECT\n  customer_id,\n  order_date,\n  order_amount,\n  SUM(order_amount) OVER (PARTITION BY customer_id ORDER BY order_date) AS running_total,\n  AVG(order_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_avg_3_orders\nFROM raw_orders\n"})}),"\n",(0,s.jsx)(n.h4,{id:"data-loading",children:"Data Loading"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Merge operation (upsert)\nMERGE INTO customers target\nUSING customer_updates source\nON target.id = source.id\nWHEN MATCHED THEN\n  UPDATE SET\n    target.email = source.email,\n    target.address = source.address,\n    target.updated_at = current_timestamp()\nWHEN NOT MATCHED THEN\n  INSERT (id, first_name, last_name, email, address, created_at, updated_at)\n  VALUES (source.id, source.first_name, source.last_name, source.email, source.address, current_timestamp(), current_timestamp())\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-building-an-etl-pipeline",children:"Hands-on Exercise: Building an ETL Pipeline"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Extract data from multiple sources"}),"\n",(0,s.jsx)(n.li,{children:"Implement transformations including cleaning, enrichment and aggregation"}),"\n",(0,s.jsx)(n.li,{children:"Load data into target Delta tables"}),"\n",(0,s.jsx)(n.li,{children:"Implement quality checks and error handling"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-just-enough-python-for-spark-sql",children:"4. Just Enough Python for Spark SQL"}),"\n",(0,s.jsx)(n.p,{children:"While Spark SQL is powerful, Python can extend its capabilities for more complex transformations and control flow."}),"\n",(0,s.jsx)(n.h4,{id:"pyspark-basics",children:"PySpark Basics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Create a DataFrame\nfrom pyspark.sql import functions as F\n\n# Read data\ndf = spark.read.format("delta").table("retail_analytics.sales")\n\n# Transformation with Python functions\ndf_transformed = df.withColumn("revenue", F.col("quantity") * F.col("price")) \\\n                   .withColumn("transaction_month", F.date_format("transaction_date", "yyyy-MM")) \\\n                   .filter(F.col("revenue") > 100)\n\n# Write back to a Delta table\ndf_transformed.write.format("delta").mode("overwrite").saveAsTable("retail_analytics.high_value_sales")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"custom-functions-with-python-udfs",children:"Custom Functions with Python UDFs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Define a Python function\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef generate_customer_segment(recency, frequency, monetary):\n    score = 0\n    if recency < 30: score += 3\n    elif recency < 90: score += 2\n    else: score += 1\n    \n    if frequency > 10: score += 3\n    elif frequency > 5: score += 2\n    else: score += 1\n    \n    if monetary > 1000: score += 3\n    elif monetary > 500: score += 2\n    else: score += 1\n    \n    segments = {\n        3: "Low Value", \n        4: "Low Value",\n        5: "Medium Value",\n        6: "Medium Value",\n        7: "High Value",\n        8: "High Value",\n        9: "High Value"\n    }\n    \n    return segments.get(score, "Unknown")\n\n# Apply UDF to DataFrame\ndf_with_segments = df.withColumn("customer_segment", \n                                 generate_customer_segment(\n                                     F.col("days_since_last_purchase"),\n                                     F.col("purchase_count"),\n                                     F.col("total_spend")\n                                 ))\n'})}),"\n",(0,s.jsx)(n.h4,{id:"combining-sql-and-python",children:"Combining SQL and Python"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Execute SQL within Python\ntop_products_df = spark.sql("""\n  SELECT \n    product_id,\n    SUM(quantity) as total_sold,\n    SUM(quantity * price) as total_revenue\n  FROM retail_analytics.sales\n  GROUP BY product_id\n  ORDER BY total_revenue DESC\n  LIMIT 10\n""")\n\n# Process results with Python\nfor product in top_products_df.collect():\n    print(f"Product {product.product_id}: Sold {product.total_sold} units, Revenue: ${product.total_revenue:.2f}")\n    \n# Create temporary view from Python DataFrame\ndf_customer_segments.createOrReplaceTempView("customer_segments")\n\n# Use the temporary view in SQL\nspark.sql("""\n  SELECT \n    customer_segment,\n    COUNT(*) as customer_count,\n    AVG(total_spend) as avg_spend\n  FROM customer_segments\n  GROUP BY customer_segment\n  ORDER BY avg_spend DESC\n""").show()\n'})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-extending-sql-with-python",children:"Hands-on Exercise: Extending SQL with Python"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a complex data cleaning routine using Python UDFs"}),"\n",(0,s.jsx)(n.li,{children:"Create a customer segmentation model with Python logic"}),"\n",(0,s.jsx)(n.li,{children:"Generate a report combining SQL aggregations and Python processing"}),"\n",(0,s.jsx)(n.li,{children:"Build a pipeline that alternates between SQL and Python steps"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-incremental-data-processing-with-structured-streaming-and-auto-loader",children:"5. Incremental Data Processing with Structured Streaming and Auto Loader"}),"\n",(0,s.jsx)(n.p,{children:"Databricks provides powerful tools for handling streaming data and incremental file ingestion."}),"\n",(0,s.jsx)(n.h4,{id:"structured-streaming",children:"Structured Streaming"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Create a streaming DataFrame from a source\nfrom pyspark.sql.functions import *\n\nstream_df = spark.readStream.format("kafka") \\\n    .option("kafka.bootstrap.servers", "kafka-server:9092") \\\n    .option("subscribe", "events_topic") \\\n    .option("startingOffsets", "latest") \\\n    .load()\n\n# Parse the JSON payload\nparsed_df = stream_df.select(\n    col("key").cast("string"),\n    from_json(col("value").cast("string"), event_schema).alias("event"),\n    col("timestamp")\n)\n\n# Extract fields from the nested structure\nprocessed_df = parsed_df.select(\n    "key",\n    "timestamp",\n    "event.user_id",\n    "event.event_type",\n    "event.item_id",\n    "event.properties"\n)\n\n# Write the stream to a Delta table\nquery = processed_df.writeStream \\\n    .format("delta") \\\n    .outputMode("append") \\\n    .option("checkpointLocation", "/mnt/checkpoints/events") \\\n    .table("events_streaming")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"auto-loader",children:"Auto Loader"}),"\n",(0,s.jsx)(n.p,{children:"Auto Loader provides a simpler way to incrementally ingest new files as they arrive in cloud storage."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Configure Auto Loader to incrementally load new files\ndf_autoloader = spark.readStream.format("cloudFiles") \\\n    .option("cloudFiles.format", "csv") \\\n    .option("cloudFiles.schemaLocation", "/mnt/schema/customer_files") \\\n    .option("cloudFiles.inferColumnTypes", "true") \\\n    .load("/mnt/landing/customer_files/")\n\n# Process and write to Delta table\ndf_autoloader.writeStream \\\n    .format("delta") \\\n    .option("checkpointLocation", "/mnt/checkpoints/customer_files") \\\n    .trigger(once=True)  # Process available files and stop\n    .table("customers_incremental")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-implementing-incremental-processing",children:"Hands-on Exercise: Implementing Incremental Processing"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Auto Loader to ingest new CSV files"}),"\n",(0,s.jsx)(n.li,{children:"Implement schema evolution handling"}),"\n",(0,s.jsx)(n.li,{children:"Create a streaming pipeline for data transformation"}),"\n",(0,s.jsx)(n.li,{children:"Configure checkpointing and failure recovery"}),"\n",(0,s.jsx)(n.li,{children:"Implement an incremental update to a reporting table"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"day-2-advanced-data-engineering",children:"Day 2: Advanced Data Engineering"}),"\n",(0,s.jsx)(n.h3,{id:"1-medallion-architecture-in-the-data-lakehouse",children:"1. Medallion Architecture in the Data Lakehouse"}),"\n",(0,s.jsx)(n.p,{children:"The Medallion architecture is a data design pattern used in the Lakehouse that organizes data into different quality tiers: Bronze (raw), Silver (validated), and Gold (aggregated/business-level)."}),"\n",(0,s.jsx)(n.h4,{id:"bronze-layer",children:"Bronze Layer"}),"\n",(0,s.jsx)(n.p,{children:"The Bronze layer contains raw, unmodified data from source systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a Bronze table\nCREATE TABLE bronze_retail_transactions (\n  payload STRING,  -- Raw JSON\n  source_file STRING,\n  ingestion_time TIMESTAMP\n)\nLOCATION '/mnt/lakehouse/bronze/retail_transactions'\n\n-- Ingest data into Bronze\nINSERT INTO bronze_retail_transactions\nSELECT \n  raw_payload,\n  input_file_name() AS source_file,\n  current_timestamp() AS ingestion_time\nFROM raw_retail_transactions\n"})}),"\n",(0,s.jsx)(n.h4,{id:"silver-layer",children:"Silver Layer"}),"\n",(0,s.jsx)(n.p,{children:"The Silver layer contains cleansed, validated data in a more structured format:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a Silver table with schema enforcement\nCREATE TABLE silver_retail_transactions (\n  transaction_id STRING,\n  store_id STRING,\n  customer_id STRING,\n  transaction_date DATE,\n  items ARRAY<STRUCT<item_id: STRING, quantity: INT, price: DOUBLE>>,\n  total_amount DOUBLE,\n  payment_method STRING,\n  ingestion_time TIMESTAMP,\n  processing_time TIMESTAMP,\n  source_file STRING\n)\nLOCATION '/mnt/lakehouse/silver/retail_transactions'\n\n-- Transform Bronze to Silver with validation\nINSERT INTO silver_retail_transactions\nSELECT\n  get_json_object(payload, '$.transaction_id') AS transaction_id,\n  get_json_object(payload, '$.store_id') AS store_id,\n  get_json_object(payload, '$.customer_id') AS customer_id,\n  to_date(get_json_object(payload, '$.transaction_date')) AS transaction_date,\n  from_json(get_json_object(payload, '$.items'), 'ARRAY<STRUCT<item_id: STRING, quantity: INT, price: DOUBLE>>') AS items,\n  cast(get_json_object(payload, '$.total_amount') AS DOUBLE) AS total_amount,\n  get_json_object(payload, '$.payment_method') AS payment_method,\n  ingestion_time,\n  current_timestamp() AS processing_time,\n  source_file\nFROM bronze_retail_transactions\nWHERE get_json_object(payload, '$.transaction_id') IS NOT NULL\n  AND to_date(get_json_object(payload, '$.transaction_date')) IS NOT NULL\n  AND cast(get_json_object(payload, '$.total_amount') AS DOUBLE) > 0\n"})}),"\n",(0,s.jsx)(n.h4,{id:"gold-layer",children:"Gold Layer"}),"\n",(0,s.jsx)(n.p,{children:"The Gold layer contains business-level aggregations and derived tables:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a Gold table for daily store performance\nCREATE TABLE gold_daily_store_performance (\n  store_id STRING,\n  transaction_date DATE,\n  transaction_count BIGINT,\n  total_revenue DOUBLE,\n  unique_customers BIGINT,\n  avg_basket_size DOUBLE,\n  avg_transaction_value DOUBLE,\n  processing_time TIMESTAMP\n)\nLOCATION '/mnt/lakehouse/gold/daily_store_performance'\n\n-- Aggregate Silver to Gold\nINSERT INTO gold_daily_store_performance\nSELECT\n  store_id,\n  transaction_date,\n  COUNT(*) AS transaction_count,\n  SUM(total_amount) AS total_revenue,\n  COUNT(DISTINCT customer_id) AS unique_customers,\n  AVG(array_size(items)) AS avg_basket_size,\n  AVG(total_amount) AS avg_transaction_value,\n  current_timestamp() AS processing_time\nFROM silver_retail_transactions\nGROUP BY store_id, transaction_date\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-implementing-a-medallion-architecture",children:"Hands-on Exercise: Implementing a Medallion Architecture"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Bronze tables to capture raw data from different sources"}),"\n",(0,s.jsx)(n.li,{children:"Implement Silver layer transformation with data validation"}),"\n",(0,s.jsx)(n.li,{children:"Create Gold tables for business metrics"}),"\n",(0,s.jsx)(n.li,{children:"Design incremental updates across all layers"}),"\n",(0,s.jsx)(n.li,{children:"Add data quality monitoring between layers"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-delta-live-tables",children:"2. Delta Live Tables"}),"\n",(0,s.jsx)(n.p,{children:"Delta Live Tables (DLT) provides a declarative framework for building reliable, maintainable, and testable data pipelines."}),"\n",(0,s.jsx)(n.h4,{id:"python-api-for-dlt",children:"Python API for DLT"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import dlt\nfrom pyspark.sql import functions as F\n\n# Bronze layer table\n@dlt.table(\n  name="bronze_customers",\n  comment="Raw customer data from source system",\n  table_properties={"quality": "bronze"}\n)\ndef bronze_customers():\n  return (\n    spark.readStream.format("cloudFiles")\n      .option("cloudFiles.format", "json")\n      .load("/mnt/landing/customers/")\n  )\n\n# Silver layer table with expectations\n@dlt.table(\n  name="silver_customers",\n  comment="Cleansed customer records",\n  table_properties={"quality": "silver"}\n)\n@dlt.expect_all({\n  "valid_email": "email IS NOT NULL AND email LIKE \'%@%.%\'",\n  "valid_age": "age > 0 AND age < 120"\n})\ndef silver_customers():\n  return (\n    dlt.read_stream("bronze_customers")\n      .select(\n        F.col("customer_id"),\n        F.lower(F.col("email")).alias("email"),\n        F.initcap(F.col("first_name")).alias("first_name"),\n        F.initcap(F.col("last_name")).alias("last_name"),\n        F.col("age"),\n        F.to_date(F.col("signup_date")).alias("signup_date")\n      )\n  )\n\n# Gold layer table\n@dlt.table(\n  name="gold_customer_demographics",\n  comment="Customer demographics for analysis",\n  table_properties={"quality": "gold"}\n)\ndef gold_customer_demographics():\n  return (\n    dlt.read("silver_customers")\n      .groupBy(\n        F.floor(F.col("age") / 10) * 10\n      )\n      .agg(\n        F.count("*").alias("customer_count"),\n        F.avg("age").alias("average_age"),\n        F.min("signup_date").alias("earliest_signup"),\n        F.max("signup_date").alias("latest_signup")\n      )\n      .withColumnRenamed("(floor((age / 10)) * 10)", "age_bracket")\n  )\n'})}),"\n",(0,s.jsx)(n.h4,{id:"sql-api-for-dlt",children:"SQL API for DLT"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'-- Bronze layer in SQL\nCREATE OR REFRESH STREAMING LIVE TABLE bronze_transactions\nCOMMENT "Raw transaction data from source system"\nTBLPROPERTIES ("quality" = "bronze")\nAS SELECT * FROM cloud_files(\n  \'/mnt/landing/transactions/\',\n  \'json\'\n);\n\n-- Silver layer with expectations\nCREATE OR REFRESH STREAMING LIVE TABLE silver_transactions (\n  CONSTRAINT valid_amount EXPECT (amount > 0),\n  CONSTRAINT valid_transaction_date EXPECT (transaction_date > \'2020-01-01\')\n)\nCOMMENT "Validated transaction records"\nTBLPROPERTIES ("quality" = "silver")\nAS\n  SELECT\n    transaction_id,\n    customer_id,\n    CAST(amount AS DOUBLE) AS amount,\n    to_date(transaction_date) AS transaction_date,\n    store_id,\n    payment_method,\n    current_timestamp() AS processing_time\n  FROM STREAM(LIVE.bronze_transactions)\n  WHERE transaction_id IS NOT NULL;\n\n-- Gold layer\nCREATE OR REFRESH LIVE TABLE gold_monthly_revenue\nCOMMENT "Monthly revenue by store"\nTBLPROPERTIES ("quality" = "gold")\nAS\n  SELECT\n    store_id,\n    date_format(transaction_date, \'yyyy-MM\') AS month,\n    COUNT(*) AS transaction_count,\n    SUM(amount) AS total_revenue,\n    COUNT(DISTINCT customer_id) AS unique_customers\n  FROM LIVE.silver_transactions\n  GROUP BY store_id, date_format(transaction_date, \'yyyy-MM\');\n'})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-building-a-dlt-pipeline",children:"Hands-on Exercise: Building a DLT Pipeline"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Design a DLT pipeline for a retail use case"}),"\n",(0,s.jsx)(n.li,{children:"Implement both Python and SQL versions"}),"\n",(0,s.jsx)(n.li,{children:"Add data quality expectations"}),"\n",(0,s.jsx)(n.li,{children:"Configure pipeline settings"}),"\n",(0,s.jsx)(n.li,{children:"Monitor pipeline execution and data quality metrics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-task-orchestration-with-databricks-jobs",children:"3. Task Orchestration with Databricks Jobs"}),"\n",(0,s.jsx)(n.p,{children:"Databricks Jobs provides a way to schedule and orchestrate complex workflows."}),"\n",(0,s.jsx)(n.h4,{id:"creating-and-scheduling-jobs",children:"Creating and Scheduling Jobs"}),"\n",(0,s.jsx)(n.p,{children:"Jobs can be created through the UI or API:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Using the Databricks Jobs API to create a job\nimport requests\nimport json\n\nurl = f"https://<databricks-instance>/api/2.1/jobs/create"\nheaders = {\n  "Authorization": f"Bearer {token}",\n  "Content-Type": "application/json"\n}\n\njob_config = {\n  "name": "Retail Analytics Pipeline",\n  "tasks": [\n    {\n      "task_key": "ingest_data",\n      "notebook_task": {\n        "notebook_path": "/Repos/team/project/notebooks/ingest_data",\n        "source": "WORKSPACE"\n      },\n      "job_cluster_key": "processing_cluster",\n      "timeout_seconds": 3600\n    },\n    {\n      "task_key": "transform_data",\n      "notebook_task": {\n        "notebook_path": "/Repos/team/project/notebooks/transform_data",\n        "source": "WORKSPACE",\n        "base_parameters": {\n          "date": "{{start_time | date: \'yyyy-MM-dd\'}}"\n        }\n      },\n      "job_cluster_key": "processing_cluster",\n      "depends_on": [\n        {\n          "task_key": "ingest_data"\n        }\n      ]\n    },\n    {\n      "task_key": "generate_reports",\n      "notebook_task": {\n        "notebook_path": "/Repos/team/project/notebooks/generate_reports",\n        "source": "WORKSPACE"\n      },\n      "job_cluster_key": "reporting_cluster",\n      "depends_on": [\n        {\n          "task_key": "transform_data"\n        }\n      ]\n    }\n  ],\n  "job_clusters": [\n    {\n      "job_cluster_key": "processing_cluster",\n      "new_cluster": {\n        "spark_version": "11.3.x-scala2.12",\n        "node_type_id": "Standard_DS3_v2",\n        "num_workers": 2,\n        "spark_conf": {\n          "spark.speculation": "true"\n        }\n      }\n    },\n    {\n      "job_cluster_key": "reporting_cluster",\n      "new_cluster": {\n        "spark_version": "11.3.x-scala2.12",\n        "node_type_id": "Standard_DS3_v2",\n        "num_workers": 1\n      }\n    }\n  ],\n  "schedule": {\n    "quartz_cron_expression": "0 0 0 * * ?",  # Daily at midnight\n    "timezone_id": "UTC"\n  },\n  "max_concurrent_runs": 1\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(job_config))\nprint(json.dumps(response.json(), indent=2))\n'})}),"\n",(0,s.jsx)(n.h4,{id:"job-parameters-and-dynamic-values",children:"Job Parameters and Dynamic Values"}),"\n",(0,s.jsx)(n.p,{children:"Jobs can use parameters to make workflows more flexible:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Static parameters defined in job config"}),"\n",(0,s.jsx)(n.li,{children:"Runtime parameters specified when triggering jobs manually"}),"\n",(0,s.jsxs)(n.li,{children:["Dynamic parameters using date expressions: ",(0,s.jsx)(n.code,{children:"{{start_time | date: 'yyyy-MM-dd'}}"})]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"error-handling-and-notifications",children:"Error Handling and Notifications"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Job configuration with notifications and retry settings\njob_config = {\n  # ... other job configuration ...\n  "email_notifications": {\n    "on_success": ["success@example.com"],\n    "on_failure": ["alerts@example.com"],\n    "no_alert_for_skipped_runs": True\n  },\n  "webhook_notifications": {\n    "on_failure": [\n      {\n        "id": "webhook-id"\n      }\n    ]\n  },\n  "retry_on_timeout": True,\n  "max_retries": 3,\n  "min_retry_interval_millis": 300000  # 5 minutes\n}\n'})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-orchestrating-workflows",children:"Hands-on Exercise: Orchestrating Workflows"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Design a multi-stage ETL workflow"}),"\n",(0,s.jsx)(n.li,{children:"Create a job with task dependencies"}),"\n",(0,s.jsx)(n.li,{children:"Configure appropriate clusters for each task"}),"\n",(0,s.jsx)(n.li,{children:"Set up scheduling and parameters"}),"\n",(0,s.jsx)(n.li,{children:"Implement error handling and notifications"}),"\n",(0,s.jsx)(n.li,{children:"Test job execution and monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-databricks-sql",children:"4. Databricks SQL"}),"\n",(0,s.jsx)(n.p,{children:"Databricks SQL provides a way to query data in the lakehouse and build dashboards for analytics."}),"\n",(0,s.jsx)(n.h4,{id:"creating-and-optimizing-sql-warehouses",children:"Creating and Optimizing SQL Warehouses"}),"\n",(0,s.jsx)(n.p,{children:"SQL Warehouses provide dedicated compute resources for SQL workloads:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Size"}),": Choose based on query complexity and concurrency requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Auto-stop"}),": Configure idle shutdown to optimize costs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scaling"}),": Set min/max clusters for elastic workloads"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spot instances"}),": Use for non-critical workloads to reduce costs"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"writing-efficient-queries",children:"Writing Efficient Queries"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Use query optimization hints\nSELECT /*+ BROADCAST(s) */ \n  c.customer_id,\n  c.name,\n  SUM(s.amount) as total_purchases\nFROM customers c\nJOIN sales s ON c.customer_id = s.customer_id\nGROUP BY c.customer_id, c.name;\n\n-- Leverage materialized views for common query patterns\nCREATE MATERIALIZED VIEW daily_sales_by_region\nAS\nSELECT \n  region,\n  transaction_date,\n  SUM(amount) as daily_sales\nFROM sales s\nJOIN stores st ON s.store_id = st.store_id\nGROUP BY region, transaction_date;\n"})}),"\n",(0,s.jsx)(n.h4,{id:"building-dashboards",children:"Building Dashboards"}),"\n",(0,s.jsx)(n.p,{children:"Databricks SQL allows you to create interactive dashboards:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create queries that produce visualization-ready results"}),"\n",(0,s.jsx)(n.li,{children:"Add visualizations (charts, tables, counters, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Organize visualizations on dashboards"}),"\n",(0,s.jsx)(n.li,{children:"Schedule dashboard refreshes"}),"\n",(0,s.jsx)(n.li,{children:"Share dashboards with stakeholders"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-analytics-with-databricks-sql",children:"Hands-on Exercise: Analytics with Databricks SQL"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create optimized queries for business reporting"}),"\n",(0,s.jsx)(n.li,{children:"Build visualizations for key metrics"}),"\n",(0,s.jsx)(n.li,{children:"Create a dashboard with multiple visualizations"}),"\n",(0,s.jsx)(n.li,{children:"Configure dashboard parameters for interactive filtering"}),"\n",(0,s.jsx)(n.li,{children:"Set up scheduled refreshes and alerts"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-managing-permissions-in-the-lakehouse",children:"5. Managing Permissions in the Lakehouse"}),"\n",(0,s.jsx)(n.p,{children:"Security is a critical aspect of data engineering. Databricks provides robust access control mechanisms."}),"\n",(0,s.jsx)(n.h4,{id:"unity-catalog",children:"Unity Catalog"}),"\n",(0,s.jsx)(n.p,{children:"Unity Catalog provides a unified governance solution for data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a catalog\nCREATE CATALOG IF NOT EXISTS retail;\n\n-- Create a schema\nCREATE SCHEMA IF NOT EXISTS retail.sales;\n\n-- Grant permissions\nGRANT USAGE ON CATALOG retail TO `data_analysts`;\nGRANT USAGE ON SCHEMA retail.sales TO `data_analysts`;\nGRANT SELECT ON TABLE retail.sales.transactions TO `data_analysts`;\n\n-- Create a user with specific permissions\nCREATE USER user1 WITH PASSWORD 'password';\nALTER USER user1 SET ROLE data_engineer;\n"})}),"\n",(0,s.jsx)(n.h4,{id:"table-access-control",children:"Table Access Control"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Grant specific permissions\nGRANT SELECT ON TABLE customers TO `analysts`;\nGRANT MODIFY ON TABLE customers TO `data_engineers`;\n\n-- Column-level security\nGRANT SELECT (customer_id, name, city, state) ON TABLE customers TO `marketing`;\n"})}),"\n",(0,s.jsx)(n.h4,{id:"dynamic-views-for-row-level-security",children:"Dynamic Views for Row-Level Security"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a secure view for row-level filtering\nCREATE VIEW sales_by_region AS\nSELECT *\nFROM sales\nWHERE region IN (SELECT allowed_regions FROM user_permissions WHERE username = current_user());\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-implementing-security",children:"Hands-on Exercise: Implementing Security"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up a Unity Catalog with multiple catalogs and schemas"}),"\n",(0,s.jsx)(n.li,{children:"Create role-based access controls"}),"\n",(0,s.jsx)(n.li,{children:"Implement column-level security for sensitive data"}),"\n",(0,s.jsx)(n.li,{children:"Create secure views for row-level filtering"}),"\n",(0,s.jsx)(n.li,{children:"Audit and monitor access patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"6-productionizing-dashboards-and-queries-on-databricks-sql",children:"6. Productionizing Dashboards and Queries on Databricks SQL"}),"\n",(0,s.jsx)(n.p,{children:"Moving dashboards and queries to production requires attention to performance, reliability, and usability."}),"\n",(0,s.jsx)(n.h4,{id:"query-optimization-techniques",children:"Query Optimization Techniques"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create an optimized table for analytics\nCREATE TABLE sales_analytics\nUSING DELTA\nPARTITIONED BY (transaction_date)\nCLUSTERED BY (customer_id) INTO 50 BUCKETS\nAS\nSELECT * FROM sales;\n\n-- Z-order optimization\nOPTIMIZE sales_analytics\nZORDER BY (store_id, product_id);\n"})}),"\n",(0,s.jsx)(n.h4,{id:"alerts-and-scheduled-queries",children:"Alerts and Scheduled Queries"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"-- Create a scheduled query\n-- (Usually done through the UI, but can be scripted via API)\nSELECT\n  current_date() AS check_date,\n  COUNT(*) AS error_count\nFROM transaction_logs\nWHERE status = 'ERROR'\n  AND log_date = current_date()\n"})}),"\n",(0,s.jsx)(n.p,{children:"Configure alerts:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Threshold-based (trigger when value exceeds threshold)"}),"\n",(0,s.jsx)(n.li,{children:"Change-based (trigger on significant changes)"}),"\n",(0,s.jsx)(n.li,{children:"Delivery via email, Slack, or webhooks"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"dashboard-parameters-and-interactivity",children:"Dashboard Parameters and Interactivity"}),"\n",(0,s.jsx)(n.p,{children:"Create dynamic dashboards with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query parameters"}),": Allow users to filter data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter mapping"}),": Connect widgets to query parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-filtering"}),": Select data in one visual to filter others"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"hands-on-exercise-production-ready-analytics",children:"Hands-on Exercise: Production-Ready Analytics"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize tables for analytical queries"}),"\n",(0,s.jsx)(n.li,{children:"Set up scheduled queries for regular reporting"}),"\n",(0,s.jsx)(n.li,{children:"Configure alerts for anomaly detection"}),"\n",(0,s.jsx)(n.li,{children:"Create interactive dashboards with parameters"}),"\n",(0,s.jsx)(n.li,{children:"Implement access controls for dashboards and queries"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary-and-additional-resources",children:"Summary and Additional Resources"}),"\n",(0,s.jsx)(n.p,{children:"This training covered essential concepts and techniques for data engineering on the Databricks Lakehouse Platform:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Delta Lake for reliable and performant data storage"}),"\n",(0,s.jsx)(n.li,{children:"ETL with Spark SQL and Python"}),"\n",(0,s.jsx)(n.li,{children:"Incremental data processing with Structured Streaming and Auto Loader"}),"\n",(0,s.jsx)(n.li,{children:"Medallion architecture for organizing data"}),"\n",(0,s.jsx)(n.li,{children:"Delta Live Tables for declarative pipeline development"}),"\n",(0,s.jsx)(n.li,{children:"Job orchestration for workflow automation"}),"\n",(0,s.jsx)(n.li,{children:"Databricks SQL for analytics and visualization"}),"\n",(0,s.jsx)(n.li,{children:"Security and access control"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"learning-resources",children:"Learning Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.databricks.com/",children:"Databricks Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://delta.io/docs/",children:"Delta Lake Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/sql-ref.html",children:"Spark SQL Reference"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.databricks.com/learn/training/",children:"Databricks Academy"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://databricks.com/resources/type/tech-talks",children:"Databricks Tech Talks"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"After completing this training, consider:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Pursuing Databricks Data Engineer Associate certification"}),"\n",(0,s.jsx)(n.li,{children:"Exploring advanced optimizations and best practices"}),"\n",(0,s.jsx)(n.li,{children:"Implementing CI/CD for Databricks workflows"}),"\n",(0,s.jsx)(n.li,{children:"Integrating ML pipelines into your data workflows"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var t=a(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);