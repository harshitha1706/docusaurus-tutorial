"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7493],{2466:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"databricks-advanced-data-engineering","title":"Advanced Data Engineering with Databricks: Optimization, Productionization, and CI/CD","description":"This advanced training material focuses on optimizing Databricks data pipelines, implementing production-grade solutions, and establishing robust CI/CD practices for data engineering projects.","source":"@site/docs/databricks-advanced-data-engineering.md","sourceDirName":".","slug":"/databricks-advanced-data-engineering","permalink":"/docusaurus-tutorial/docs/databricks-advanced-data-engineering","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/databricks-advanced-data-engineering.md","tags":[],"version":"current","frontMatter":{}}');var i=t(4848),s=t(8453);const r={},o="Advanced Data Engineering with Databricks: Optimization, Productionization, and CI/CD",l={},c=[{value:"1. Optimizing Spark and Delta Lake Workloads",id:"1-optimizing-spark-and-delta-lake-workloads",level:2},{value:"Performance Tuning for Spark Applications",id:"performance-tuning-for-spark-applications",level:3},{value:"Cluster Configuration Optimization",id:"cluster-configuration-optimization",level:4},{value:"Query Optimization Techniques",id:"query-optimization-techniques",level:4},{value:"Analyzing Performance with Spark UI",id:"analyzing-performance-with-spark-ui",level:4},{value:"Hands-on Exercise: Performance Tuning",id:"hands-on-exercise-performance-tuning",level:4},{value:"Delta Lake Optimization Strategies",id:"delta-lake-optimization-strategies",level:3},{value:"Physical Layout Optimization",id:"physical-layout-optimization",level:4},{value:"Liquid Clustering (Preview)",id:"liquid-clustering-preview",level:4},{value:"Optimizing for Specific Query Patterns",id:"optimizing-for-specific-query-patterns",level:4},{value:"Vacuum and Retention Policies",id:"vacuum-and-retention-policies",level:4},{value:"Hands-on Exercise: Delta Lake Optimization",id:"hands-on-exercise-delta-lake-optimization",level:4},{value:"2. Advanced Data Pipeline Design Patterns",id:"2-advanced-data-pipeline-design-patterns",level:2},{value:"Incremental Data Processing Patterns",id:"incremental-data-processing-patterns",level:3},{value:"Slowly Changing Dimensions (SCD)",id:"slowly-changing-dimensions-scd",level:4},{value:"Change Data Capture (CDC)",id:"change-data-capture-cdc",level:4},{value:"Incremental Aggregation",id:"incremental-aggregation",level:4},{value:"Hands-on Exercise: Implementing Advanced Processing Patterns",id:"hands-on-exercise-implementing-advanced-processing-patterns",level:4},{value:"Resilient Pipeline Design",id:"resilient-pipeline-design",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:4},{value:"Data Quality Monitoring",id:"data-quality-monitoring",level:4},{value:"Circuit Breakers and Failover",id:"circuit-breakers-and-failover",level:4},{value:"Idempotent Operations",id:"idempotent-operations",level:4},{value:"Hands-on Exercise: Building Resilient Pipelines",id:"hands-on-exercise-building-resilient-pipelines",level:4},{value:"3. CI/CD for Data Engineering on Databricks",id:"3-cicd-for-data-engineering-on-databricks",level:2},{value:"Version Control and Development Workflows",id:"version-control-and-development-workflows",level:3},{value:"Git Integration with Databricks Repos",id:"git-integration-with-databricks-repos",level:4},{value:"Databricks Asset Bundles (DABs)",id:"databricks-asset-bundles-dabs",level:4},{value:"Testing Strategies for Data Pipelines",id:"testing-strategies-for-data-pipelines",level:3},{value:"Unit Testing",id:"unit-testing",level:4},{value:"Integration Testing",id:"integration-testing",level:4},{value:"Data Quality Testing",id:"data-quality-testing",level:4},{value:"Hands-on Exercise: Testing Data Pipelines",id:"hands-on-exercise-testing-data-pipelines",level:4},{value:"Automating Deployments",id:"automating-deployments",level:3},{value:"CI/CD Pipeline Setup",id:"cicd-pipeline-setup",level:4},{value:"Multi-Environment Configuration",id:"multi-environment-configuration",level:4},{value:"Infrastructure as Code",id:"infrastructure-as-code",level:4},{value:"Hands-on Exercise: Setting up Automated Deployments",id:"hands-on-exercise-setting-up-automated-deployments",level:4},{value:"4. Monitoring and Observability",id:"4-monitoring-and-observability",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Tracking Job and Query Performance",id:"tracking-job-and-query-performance",level:4},{value:"Performance Dashboard",id:"performance-dashboard",level:4},{value:"Data Quality Monitoring",id:"data-quality-monitoring-1",level:3},{value:"Building Data Quality Dashboards",id:"building-data-quality-dashboards",level:4},{value:"Anomaly Detection for Data Pipelines",id:"anomaly-detection-for-data-pipelines",level:4},{value:"Alerting and Notification Systems",id:"alerting-and-notification-systems",level:3},{value:"Setting Up Alerts",id:"setting-up-alerts",level:4},{value:"Integration with Incident Management",id:"integration-with-incident-management",level:4},{value:"Hands-on Exercise: Implementing Monitoring and Alerting",id:"hands-on-exercise-implementing-monitoring-and-alerting",level:4},{value:"5. Advanced Optimization Techniques",id:"5-advanced-optimization-techniques",level:2},{value:"Query and Pipeline Optimization Patterns",id:"query-and-pipeline-optimization-patterns",level:3},{value:"Cost-Based Optimization",id:"cost-based-optimization",level:4},{value:"Dynamic Partition Pruning",id:"dynamic-partition-pruning",level:4},{value:"Skew Handling",id:"skew-handling",level:4},{value:"Cache Optimization",id:"cache-optimization",level:4},{value:"Hands-on Exercise: Advanced Query Optimization",id:"hands-on-exercise-advanced-query-optimization",level:4},{value:"Resource Management and Isolation",id:"resource-management-and-isolation",level:3},{value:"Multi-Tenant Resource Allocation",id:"multi-tenant-resource-allocation",level:4},{value:"Resource Quotas and Limits",id:"resource-quotas-and-limits",level:4},{value:"Hands-on Exercise: Resource Management",id:"hands-on-exercise-resource-management",level:4},{value:"6. Security and Governance Best Practices",id:"6-security-and-governance-best-practices",level:2},{value:"Advanced Security Configuration",id:"advanced-security-configuration",level:3},{value:"Unity Catalog Setup",id:"unity-catalog-setup",level:4},{value:"Advanced Access Control",id:"advanced-access-control",level:4},{value:"Encryption and Key Management",id:"encryption-and-key-management",level:4},{value:"Hands-on Exercise: Security Implementation",id:"hands-on-exercise-security-implementation",level:4},{value:"Data Governance and Compliance",id:"data-governance-and-compliance",level:3},{value:"Data Lineage Tracking",id:"data-lineage-tracking",level:4},{value:"Data Cataloging and Documentation",id:"data-cataloging-and-documentation",level:4},{value:"Regulatory Compliance",id:"regulatory-compliance",level:4},{value:"Hands-on Exercise: Data Governance Implementation",id:"hands-on-exercise-data-governance-implementation",level:4},{value:"Summary and Key Takeaways",id:"summary-and-key-takeaways",level:2},{value:"Recommended Next Steps",id:"recommended-next-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"advanced-data-engineering-with-databricks-optimization-productionization-and-cicd",children:"Advanced Data Engineering with Databricks: Optimization, Productionization, and CI/CD"})}),"\n",(0,i.jsx)(n.p,{children:"This advanced training material focuses on optimizing Databricks data pipelines, implementing production-grade solutions, and establishing robust CI/CD practices for data engineering projects."}),"\n",(0,i.jsx)(n.h2,{id:"1-optimizing-spark-and-delta-lake-workloads",children:"1. Optimizing Spark and Delta Lake Workloads"}),"\n",(0,i.jsx)(n.h3,{id:"performance-tuning-for-spark-applications",children:"Performance Tuning for Spark Applications"}),"\n",(0,i.jsx)(n.h4,{id:"cluster-configuration-optimization",children:"Cluster Configuration Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Proper cluster sizing and configuration is crucial for performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-scala",children:'// Example Cluster Configuration\n{\n  "num_workers": 10,\n  "spark_version": "11.3.x-scala2.12",\n  "node_type_id": "Standard_DS4_v2",\n  "spark_conf": {\n    "spark.databricks.delta.optimizeWrite.enabled": "true",\n    "spark.databricks.delta.autoCompact.enabled": "true",\n    "spark.sql.adaptive.enabled": "true",\n    "spark.sql.adaptive.coalescePartitions.enabled": "true",\n    "spark.sql.files.maxPartitionBytes": "134217728",\n    "spark.sql.shuffle.partitions": "200"\n  },\n  "aws_attributes": {\n    "instance_profile_arn": "arn:aws:iam::XXXXXXXXXXXX:instance-profile/databricks-role",\n    "availability": "SPOT",\n    "zone_id": "us-west-2a"\n  },\n  "autoscale": {\n    "min_workers": 5,\n    "max_workers": 20\n  }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Key configuration considerations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Worker type"}),": Choose based on workload (memory-optimized for large joins, compute-optimized for transformations)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Autoscaling"}),": Configure min/max workers based on workload variability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spark parameters"}),": Tune based on data size and processing patterns"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"query-optimization-techniques",children:"Query Optimization Techniques"}),"\n",(0,i.jsx)(n.p,{children:"Understanding the Spark execution plan is essential for optimization:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# View the execution plan\ndf = spark.sql("SELECT * FROM sales JOIN customers ON sales.customer_id = customers.id")\ndf.explain(True)\n\n# Analyze query metrics\ndf.select("customer_id", "amount").groupBy("customer_id").sum("amount").collect()\nspark.sql("ANALYZE TABLE sales COMPUTE STATISTICS")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Common optimization techniques:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Broadcast hints for small tables"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Broadcast the smaller table in a join\nspark.sql("""\nSELECT /*+ BROADCAST(customers) */ \n  customers.name, \n  SUM(sales.amount) as total\nFROM sales\nJOIN customers ON sales.customer_id = customers.id\nGROUP BY customers.name\n""")\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Repartitioning for better parallelism"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Repartition before a heavy operation\ndf = df.repartition(200, "customer_id").cache()\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caching frequently used DataFrames"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Cache a DataFrame that will be used multiple times\nfrequent_df = spark.table("high_value_customers").cache()\nfrequent_df.count()  # Materialize the cache\n'})}),"\n",(0,i.jsx)(n.h4,{id:"analyzing-performance-with-spark-ui",children:"Analyzing Performance with Spark UI"}),"\n",(0,i.jsx)(n.p,{children:"The Spark UI provides detailed insights into job execution:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Jobs tab"}),": Overview of Spark jobs and their stages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stages tab"}),": Detailed view of task distribution and skew"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage tab"}),": Information about cached data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SQL tab"}),": SQL query execution details"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Executors tab"}),": Executor resource utilization"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Key metrics to monitor:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task skew"}),": Identify tasks taking much longer than others"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spill"}),": Data spilled to disk indicates memory pressure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Shuffle read/write"}),": Large shuffles can be performance bottlenecks"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-performance-tuning",children:"Hands-on Exercise: Performance Tuning"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Analyze a slow-running query using the Spark UI"}),"\n",(0,i.jsx)(n.li,{children:"Identify bottlenecks (skew, inefficient joins, etc.)"}),"\n",(0,i.jsx)(n.li,{children:"Apply optimization techniques (broadcast joins, partitioning)"}),"\n",(0,i.jsx)(n.li,{children:"Measure performance improvement"}),"\n",(0,i.jsx)(n.li,{children:"Document best practices for your specific workload patterns"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"delta-lake-optimization-strategies",children:"Delta Lake Optimization Strategies"}),"\n",(0,i.jsx)(n.h4,{id:"physical-layout-optimization",children:"Physical Layout Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Optimizing how data is stored on disk can dramatically improve query performance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Z-ordering for multi-dimensional filtering\nOPTIMIZE sales_data\nZORDER BY (date, region, product_category);\n\n-- Partitioning for high-cardinality filtering\nCREATE TABLE sales_partitioned\nPARTITIONED BY (date)\nAS SELECT * FROM sales_data;\n\n-- Data skipping statistics\nANALYZE TABLE sales_partitioned COMPUTE STATISTICS;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"liquid-clustering-preview",children:"Liquid Clustering (Preview)"}),"\n",(0,i.jsx)(n.p,{children:"Liquid Clustering provides automatic data organization advantages:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Create a table with Liquid Clustering\nCREATE TABLE sales_data_clustered (\n  transaction_id STRING,\n  customer_id STRING,\n  product_id STRING,\n  store_id STRING,\n  transaction_date DATE,\n  amount DOUBLE\n)\nCLUSTER BY (transaction_date, store_id);\n\n-- Convert existing table to use Liquid Clustering\nALTER TABLE sales_data\nCLUSTER BY (transaction_date, store_id);\n"})}),"\n",(0,i.jsx)(n.h4,{id:"optimizing-for-specific-query-patterns",children:"Optimizing for Specific Query Patterns"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Create and maintain materialized views for common query patterns\nCREATE MATERIALIZED VIEW daily_sales_by_region\nAS SELECT \n  transaction_date,\n  region,\n  SUM(amount) as daily_total\nFROM sales s\nJOIN stores st ON s.store_id = st.id\nGROUP BY transaction_date, region;\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW daily_sales_by_region;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"vacuum-and-retention-policies",children:"Vacuum and Retention Policies"}),"\n",(0,i.jsx)(n.p,{children:"Managing historical versions and cleaning up old files:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- View retention period\nSHOW TBLPROPERTIES sales_data LIKE 'delta.logRetentionDuration';\n\n-- Change retention period\nALTER TABLE sales_data \nSET TBLPROPERTIES ('delta.logRetentionDuration' = '30 days');\n\n-- Vacuum old files (dry run)\nVACUUM sales_data DRY RUN;\n\n-- Vacuum old files (actual)\nVACUUM sales_data RETAIN 168 HOURS;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-delta-lake-optimization",children:"Hands-on Exercise: Delta Lake Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Analyze query patterns on a dataset"}),"\n",(0,i.jsx)(n.li,{children:"Implement appropriate physical optimization strategies"}),"\n",(0,i.jsx)(n.li,{children:"Configure and test Liquid Clustering"}),"\n",(0,i.jsx)(n.li,{children:"Create materialized views for common query patterns"}),"\n",(0,i.jsx)(n.li,{children:"Set up appropriate data retention policies"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-advanced-data-pipeline-design-patterns",children:"2. Advanced Data Pipeline Design Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"incremental-data-processing-patterns",children:"Incremental Data Processing Patterns"}),"\n",(0,i.jsx)(n.h4,{id:"slowly-changing-dimensions-scd",children:"Slowly Changing Dimensions (SCD)"}),"\n",(0,i.jsx)(n.p,{children:"Implementing SCD Type 2 with Delta Lake:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# SCD Type 2 implementation\nfrom pyspark.sql.functions import current_timestamp, lit\n\n# 1. Identify changed records\ncustomers_current = spark.table("customers")\ncustomers_new = spark.table("customers_updates")\n\n# Join to find changes\nmatched_records = customers_current.alias("current").join(\n    customers_new.alias("new"),\n    customers_current.customer_id == customers_new.customer_id,\n    "inner"\n).where("""\n    current.email != new.email OR\n    current.address != new.address OR\n    current.phone != new.phone\n""")\n\n# 2. Expire current records\nto_expire = matched_records.select(\n    "current.customer_id",\n    "current.name",\n    "current.email",\n    "current.address",\n    "current.phone",\n    "current.valid_from",\n    current_timestamp().alias("valid_to"),\n    lit(False).alias("is_current")\n)\n\n# 3. Insert new records\nto_insert = matched_records.select(\n    "new.customer_id",\n    "new.name",\n    "new.email",\n    "new.address",\n    "new.phone",\n    current_timestamp().alias("valid_from"),\n    lit(None).alias("valid_to"),\n    lit(True).alias("is_current")\n)\n\n# 4. Insert records for new customers\nnew_customers = customers_new.join(\n    customers_current,\n    customers_new.customer_id == customers_current.customer_id,\n    "left_anti"\n).select(\n    "customer_id",\n    "name",\n    "email",\n    "address",\n    "phone",\n    current_timestamp().alias("valid_from"),\n    lit(None).alias("valid_to"),\n    lit(True).alias("is_current")\n)\n\n# 5. Update the table with MERGE\nfrom delta.tables import DeltaTable\n\ncustomers_table = DeltaTable.forName(spark, "customers")\n\n# Expire current records\ncustomers_table.alias("customers").merge(\n    to_expire.alias("updates"),\n    "customers.customer_id = updates.customer_id AND customers.is_current = true"\n).whenMatched().updateAll().execute()\n\n# Insert new versions and new customers\ncustomers_table.alias("customers").merge(\n    to_insert.union(new_customers).alias("updates"),\n    "customers.customer_id = updates.customer_id AND customers.valid_from = updates.valid_from"\n).whenNotMatched().insertAll().execute()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"change-data-capture-cdc",children:"Change Data Capture (CDC)"}),"\n",(0,i.jsx)(n.p,{children:"Processing CDC records with Delta Live Tables:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import dlt\nfrom pyspark.sql.functions import col\n\n@dlt.table(\n  name="bronze_cdc_records",\n  comment="Raw CDC records from source system"\n)\ndef bronze_cdc_records():\n  return (\n    spark.readStream.format("cloudFiles")\n      .option("cloudFiles.format", "json")\n      .load("/mnt/landing/cdc/")\n  )\n\n@dlt.table(\n  name="silver_customers",\n  comment="Latest customer data processed from CDC feeds"\n)\ndef silver_customers():\n  cdc_records = dlt.read("bronze_cdc_records")\n  \n  # Process inserts\n  inserts = cdc_records.filter(col("operation") == "INSERT").select(\n    col("after.*"),\n    col("timestamp").alias("last_updated")\n  )\n  \n  # Process updates\n  updates = cdc_records.filter(col("operation") == "UPDATE").select(\n    col("after.*"),\n    col("timestamp").alias("last_updated")\n  )\n  \n  # Process deletes\n  deletes = cdc_records.filter(col("operation") == "DELETE").select(\n    col("before.customer_id"),\n    col("timestamp").alias("deleted_at")\n  )\n  \n  # Apply changes to target table\n  return (\n    dlt.apply_changes(\n      target = "silver_customers",\n      source = inserts.union(updates),\n      keys = ["customer_id"],\n      sequence_by = col("last_updated"),\n      apply_as_deletes = deletes,\n      except_column_list = ["deleted_at"]\n    )\n  )\n'})}),"\n",(0,i.jsx)(n.h4,{id:"incremental-aggregation",children:"Incremental Aggregation"}),"\n",(0,i.jsx)(n.p,{children:"Computing aggregations incrementally with Structured Streaming:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Stream of event data\nevents = spark.readStream.format("delta").table("events")\n\n# Define watermark for late data\nevents_with_watermark = events.withWatermark("event_time", "1 hour")\n\n# Compute sliding window aggregations\nwindowed_counts = events_with_watermark.groupBy(\n    F.window("event_time", "1 hour", "15 minutes"),\n    "event_type"\n).count()\n\n# Write results to Delta table\nquery = windowed_counts.writeStream \\\n    .format("delta") \\\n    .outputMode("complete") \\\n    .option("checkpointLocation", "/mnt/checkpoints/windowed_counts") \\\n    .table("hourly_event_counts")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-implementing-advanced-processing-patterns",children:"Hands-on Exercise: Implementing Advanced Processing Patterns"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement an SCD Type 2 pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Build a CDC processing pipeline using Delta Live Tables"}),"\n",(0,i.jsx)(n.li,{children:"Implement incremental aggregations with watermarking"}),"\n",(0,i.jsx)(n.li,{children:"Test handling of late-arriving data"}),"\n",(0,i.jsx)(n.li,{children:"Measure performance compared to full reprocessing"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"resilient-pipeline-design",children:"Resilient Pipeline Design"}),"\n",(0,i.jsx)(n.h4,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(n.p,{children:"Implementing robust error handling in pipelines:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Error handling with exception capture\ndef process_with_error_handling(batch_df, batch_id):\n    try:\n        # Process the batch\n        processed_df = transform_data(batch_df)\n        \n        # Write successfully processed records\n        processed_df.write.format("delta").mode("append").saveAsTable("processed_data")\n        \n        # Track success\n        log_success(batch_id, processed_df.count())\n        \n    except Exception as e:\n        # Log the error\n        log_error(batch_id, str(e))\n        \n        # Write failed records to error table\n        batch_df.withColumn("error", F.lit(str(e))) \\\n            .withColumn("error_time", F.current_timestamp()) \\\n            .write.format("delta").mode("append").saveAsTable("failed_records")\n        \n        # Optionally raise to fail the job\n        # raise e\n\n# Use in a streaming context\nquery = df.writeStream \\\n    .foreachBatch(process_with_error_handling) \\\n    .option("checkpointLocation", "/mnt/checkpoints/process") \\\n    .start()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"data-quality-monitoring",children:"Data Quality Monitoring"}),"\n",(0,i.jsx)(n.p,{children:"Implementing data quality checks with expectations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import dlt\nfrom pyspark.sql import functions as F\n\n# Define data quality rules\n@dlt.table(\n  name="silver_transactions"\n)\n@dlt.expect_all({\n  "valid_amount": "amount > 0",\n  "valid_transaction_id": "transaction_id IS NOT NULL",\n  "valid_date": "transaction_date <= current_date()"\n})\n@dlt.expect_or_drop(\n  "complete_record", "customer_id IS NOT NULL AND store_id IS NOT NULL"\n)\ndef silver_transactions():\n  return (\n    dlt.read("bronze_transactions")\n      .withColumn("amount", F.col("amount").cast("double"))\n      .withColumn("transaction_date", F.to_date(F.col("transaction_date")))\n  )\n\n# Track data quality metrics\n@dlt.table(\n  name="data_quality_metrics"\n)\ndef data_quality_metrics():\n  metrics = dlt.read_metric_history("silver_transactions.valid_amount,silver_transactions.valid_transaction_id,silver_transactions.valid_date,silver_transactions.complete_record")\n  \n  return (\n    metrics\n      .groupBy("flow_progress.flow_name", "flow_progress.flow_id", "flow_progress.run_id", "metric_name")\n      .agg(\n        F.min("metric_value.pipeline_progress.data_quality.dropped_records").alias("dropped_records"),\n        F.min("metric_value.pipeline_progress.data_quality.expectations.failed_records").alias("failed_records"),\n        F.min("metric_value.pipeline_progress.data_quality.expectations.passed_records").alias("passed_records")\n      )\n  )\n'})}),"\n",(0,i.jsx)(n.h4,{id:"circuit-breakers-and-failover",children:"Circuit Breakers and Failover"}),"\n",(0,i.jsx)(n.p,{children:"Implementing circuit breakers to prevent cascading failures:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Simple circuit breaker implementation\nclass CircuitBreaker:\n    def __init__(self, failure_threshold=3, reset_timeout=300):\n        self.failure_count = 0\n        self.failure_threshold = failure_threshold\n        self.reset_timeout = reset_timeout\n        self.last_failure_time = 0\n        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN\n    \n    def is_allowed(self):\n        current_time = time.time()\n        \n        # Check if circuit needs to be reset\n        if self.state == "OPEN" and current_time - self.last_failure_time > self.reset_timeout:\n            self.state = "HALF_OPEN"\n            print("Circuit half-open, allowing test request")\n        \n        return self.state != "OPEN"\n    \n    def record_success(self):\n        if self.state == "HALF_OPEN":\n            self.state = "CLOSED"\n            self.failure_count = 0\n            print("Circuit closed")\n    \n    def record_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.failure_count >= self.failure_threshold:\n            self.state = "OPEN"\n            print(f"Circuit open, blocking requests for {self.reset_timeout} seconds")\n\n# Usage in a data pipeline\ncircuit_breaker = CircuitBreaker(failure_threshold=3, reset_timeout=300)\n\ndef process_batch_with_circuit_breaker(batch_df, batch_id):\n    if not circuit_breaker.is_allowed():\n        print("Circuit breaker open, skipping batch")\n        return\n    \n    try:\n        # Process the batch\n        result_df = process_data(batch_df)\n        \n        # Record success\n        circuit_breaker.record_success()\n        \n        # Continue with normal processing\n        result_df.write.format("delta").mode("append").saveAsTable("processed_data")\n    except Exception as e:\n        # Record failure\n        circuit_breaker.record_failure()\n        \n        # Handle error\n        print(f"Error processing batch {batch_id}: {str(e)}")\n        \n        # Write to error table\n        batch_df.withColumn("error", F.lit(str(e))) \\\n            .write.format("delta").mode("append").saveAsTable("error_data")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"idempotent-operations",children:"Idempotent Operations"}),"\n",(0,i.jsx)(n.p,{children:"Ensuring operations can be safely retried:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Idempotent write with deduplication\ndef idempotent_write(df, table_name, unique_key_columns):\n    """\n    Write data to a table in an idempotent way by deduplicating based on key columns.\n    """\n    # Generate a batch ID for this operation\n    batch_id = str(uuid.uuid4())\n    \n    # Add batch metadata\n    df_with_metadata = df.withColumn("_batch_id", F.lit(batch_id)) \\\n                         .withColumn("_inserted_at", F.current_timestamp())\n    \n    # Write to a staging table\n    staging_table = f"{table_name}_staging_{batch_id.replace(\'-\', \'_\')}"\n    df_with_metadata.write.format("delta").saveAsTable(staging_table)\n    \n    # Perform idempotent merge into target\n    key_condition = " AND ".join([f"target.{col} = source.{col}" for col in unique_key_columns])\n    \n    spark.sql(f"""\n    MERGE INTO {table_name} target\n    USING {staging_table} source\n    ON {key_condition}\n    WHEN MATCHED THEN\n        UPDATE SET *\n    WHEN NOT MATCHED THEN\n        INSERT *\n    """)\n    \n    # Clean up staging table\n    spark.sql(f"DROP TABLE {staging_table}")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-building-resilient-pipelines",children:"Hands-on Exercise: Building Resilient Pipelines"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement comprehensive error handling in a pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Add data quality monitoring with expectations"}),"\n",(0,i.jsx)(n.li,{children:"Set up circuit breakers for external dependencies"}),"\n",(0,i.jsx)(n.li,{children:"Create idempotent write operations"}),"\n",(0,i.jsx)(n.li,{children:"Test recovery from various failure scenarios"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3-cicd-for-data-engineering-on-databricks",children:"3. CI/CD for Data Engineering on Databricks"}),"\n",(0,i.jsx)(n.h3,{id:"version-control-and-development-workflows",children:"Version Control and Development Workflows"}),"\n",(0,i.jsx)(n.h4,{id:"git-integration-with-databricks-repos",children:"Git Integration with Databricks Repos"}),"\n",(0,i.jsx)(n.p,{children:"Setting up and using Git repositories in Databricks:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Connect a repository"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"In the Databricks UI: Repos > Add Repo"}),"\n",(0,i.jsx)(n.li,{children:"Provide Git URL and credentials"}),"\n",(0,i.jsx)(n.li,{children:"Clone the repository into Databricks"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Working with branches"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Create new branches for features"}),"\n",(0,i.jsx)(n.li,{children:"Submit pull requests for review"}),"\n",(0,i.jsx)(n.li,{children:"Merge changes to main branch"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Development workflow"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Create a feature branch"}),"\n",(0,i.jsx)(n.li,{children:"Develop and test in notebooks"}),"\n",(0,i.jsx)(n.li,{children:"Commit changes and push to remote"}),"\n",(0,i.jsx)(n.li,{children:"Create PR for review"}),"\n",(0,i.jsx)(n.li,{children:"Merge to main branch"}),"\n",(0,i.jsx)(n.li,{children:"Deploy to production"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"databricks-asset-bundles-dabs",children:"Databricks Asset Bundles (DABs)"}),"\n",(0,i.jsx)(n.p,{children:"DABs provide a way to package and deploy Databricks assets:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Databricks CLI\npip install databricks-cli\n\n# Initialize DAB configuration\ndatabricks bundle init\n\n# Bundle structure\nmy_project/\n\u251c\u2500\u2500 .databricks/\n\u2502   \u2514\u2500\u2500 bundle.yml\n\u251c\u2500\u2500 resources/\n\u2502   \u251c\u2500\u2500 notebooks/\n\u2502   \u2502   \u2514\u2500\u2500 etl_pipeline.py\n\u2502   \u251c\u2500\u2500 jobs/\n\u2502   \u2502   \u2514\u2500\u2500 daily_etl.yml\n\u2502   \u2514\u2500\u2500 pipelines/\n\u2502       \u2514\u2500\u2500 dlt_pipeline.yml\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_etl_pipeline.py\n\u2514\u2500\u2500 README.md\n\n# Deploy bundle\ndatabricks bundle deploy\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Example ",(0,i.jsx)(n.code,{children:"bundle.yml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'bundle:\n  name: retail_analytics\n\nworkspace:\n  host: ${DATABRICKS_HOST}\n  token: ${DATABRICKS_TOKEN}\n\nresources:\n  pipelines:\n    retail_analytics_pipeline:\n      path: resources/pipelines/dlt_pipeline.yml\n      target: dev\n      continuous: false\n      development: true\n      photon: true\n      channels:\n        - current\n      configuration:\n        - key: source_path\n          value: /mnt/landing/retail\n  \n  jobs:\n    daily_etl:\n      path: resources/jobs/daily_etl.yml\n      schedule:\n        quartz_cron_expression: "0 0 2 * * ?"\n        timezone_id: "UTC"\n      email_notifications:\n        on_failure:\n          - team@example.com\n      tags:\n        environment: dev\n'})}),"\n",(0,i.jsx)(n.h3,{id:"testing-strategies-for-data-pipelines",children:"Testing Strategies for Data Pipelines"}),"\n",(0,i.jsx)(n.h4,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,i.jsx)(n.p,{children:"Writing unit tests for transformation logic:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# tests/test_transformations.py\nimport unittest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom src.transformations import clean_customer_data\n\nclass TestTransformations(unittest.TestCase):\n    \n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder \\\n            .appName("UnitTests") \\\n            .getOrCreate()\n    \n    def test_clean_customer_data(self):\n        # Define test schema\n        schema = StructType([\n            StructField("customer_id", StringType(), True),\n            StructField("name", StringType(), True),\n            StructField("email", StringType(), True),\n            StructField("age", IntegerType(), True)\n        ])\n        \n        # Create test data\n        test_data = [\n            ("C001", "john smith", "john.smith@example.com", 35),\n            ("C002", "JANE DOE", "jane.doe@example.com", -5),\n            ("C003", "  Bob Jones  ", "invalid-email", 150),\n            ("C004", "Alice Brown", None, 25)\n        ]\n        \n        test_df = self.spark.createDataFrame(test_data, schema)\n        \n        # Apply transformation\n        result_df = clean_customer_data(test_df)\n        \n        # Convert to list for assertions\n        results = result_df.collect()\n        \n        # Verify results\n        self.assertEqual(len(results), 4)\n        \n        # Check case normalization\n        self.assertEqual(results[0].name, "John Smith")\n        self.assertEqual(results[1].name, "Jane Doe")\n        \n        # Check age constraints\n        self.assertEqual(results[1].age, 0)  # Should be clamped to minimum\n        self.assertEqual(results[2].age, 120)  # Should be clamped to maximum\n        \n        # Check email validation\n        self.assertEqual(results[2].valid_email, False)\n        self.assertEqual(results[0].valid_email, True)\n        self.assertEqual(results[3].valid_email, False)  # Null email\n        \n        # Check string trimming\n        self.assertEqual(results[2].name, "Bob Jones")  # Spaces trimmed\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\nif __name__ == "__main__":\n    unittest.main()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,i.jsx)(n.p,{children:"Testing pipeline components together:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# tests/test_pipeline_integration.py\nimport unittest\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport os\n\nclass TestPipelineIntegration(unittest.TestCase):\n    \n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder \\\n            .appName("IntegrationTests") \\\n            .master("local[2]") \\\n            .config("spark.sql.warehouse.dir", "/tmp/warehouse") \\\n            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \\\n            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \\\n            .getOrCreate()\n        \n        # Set up test data\n        cls._create_test_data()\n    \n    @classmethod\n    def _create_test_data(cls):\n        # Create bronze test data\n        cls.spark.sql("CREATE DATABASE IF NOT EXISTS test_db")\n        \n        # Create test customer data\n        customer_df = cls.spark.createDataFrame([\n            ("C001", "John Smith", "john@example.com", 35),\n            ("C002", "Jane Doe", "jane@example.com", 42)\n        ], ["customer_id", "name", "email", "age"])\n        \n        customer_df.write.format("delta").mode("overwrite").saveAsTable("test_db.customers_bronze")\n        \n        # Create test transaction data\n        transaction_df = cls.spark.createDataFrame([\n            ("T001", "C001", "2023-01-15", 100.50),\n            ("T002", "C002", "2023-01-16", 200.75),\n            ("T003", "C001", "2023-01-17", 50.25)\n        ], ["transaction_id", "customer_id", "transaction_date", "amount"])\n        \n        transaction_df.write.format("delta").mode("overwrite").saveAsTable("test_db.transactions_bronze")\n    \n    def test_end_to_end_pipeline(self):\n        # Import the pipeline functions\n        import sys\n        sys.path.append("../src")\n        from pipeline import process_customers, process_transactions, create_customer_summary\n        \n        # Run the pipeline components\n        process_customers("test_db.customers_bronze", "test_db.customers_silver")\n        process_transactions("test_db.transactions_bronze", "test_db.transactions_silver")\n        create_customer_summary("test_db.customers_silver", "test_db.transactions_silver", "test_db.customer_summary")\n        \n        # Verify the results\n        customer_summary = self.spark.table("test_db.customer_summary")\n        results = customer_summary.collect()\n        \n        # Check results match expectations\n        self.assertEqual(len(results), 2)\n        \n        # Find C001 record\n        c001_record = next(r for r in results if r.customer_id == "C001")\n        self.assertEqual(c001_record.transaction_count, 2)\n        self.assertEqual(c001_record.total_spent, 150.75)\n        \n        # Find C002 record\n        c002_record = next(r for r in results if r.customer_id == "C002")\n        self.assertEqual(c002_record.transaction_count, 1)\n        self.assertEqual(c002_record.total_spent, 200.75)\n    \n    @classmethod\n    def tearDownClass(cls):\n        # Clean up test database\n        cls.spark.sql("DROP DATABASE IF EXISTS test_db CASCADE")\n        cls.spark.stop()\n\nif __name__ == "__main__":\n    unittest.main()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"data-quality-testing",children:"Data Quality Testing"}),"\n",(0,i.jsx)(n.p,{children:"Testing data quality requirements:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# tests/test_data_quality.py\nimport unittest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nclass TestDataQuality(unittest.TestCase):\n    \n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder \\\n            .appName("DataQualityTests") \\\n            .getOrCreate()\n    \n    def test_customer_data_quality(self):\n        # Read the customer data\n        customers = self.spark.table("customers_silver")\n        \n        # Test for nulls in required fields\n        null_customer_ids = customers.filter(col("customer_id").isNull()).count()\n        self.assertEqual(null_customer_ids, 0, "There should be no null customer IDs")\n        \n        # Test email format\n        invalid_emails = customers.filter(\n            ~col("email").rlike("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$")\n        ).count()\n        self.assertEqual(invalid_emails, 0, "All emails should be in valid format")\n        \n        # Test age ranges\n        invalid_ages = customers.filter(\n            (col("age") < 0) | (col("age") > 120)\n        ).count()\n        self.assertEqual(invalid_ages, 0, "All ages should be between 0 and 120")\n        \n        # Test uniqueness constraints\n        total_count = customers.count()\n        distinct_count = customers.select("customer_id").distinct().count()\n        self.assertEqual(total_count, distinct_count, "Customer IDs should be unique")\n    \n    def test_transaction_data_quality(self):\n        # Read the transaction data\n        transactions = self.spark.table("transactions_silver")\n        \n        # Test for nulls in required fields\n        null_transaction_ids = transactions.filter(col("transaction_id").isNull()).count()\n        self.assertEqual(null_transaction_ids, 0, "There should be no null transaction IDs")\n        \n        # Test amount ranges\n        negative_amounts = transactions.filter(col("amount") < 0).count()\n        self.assertEqual(negative_amounts, 0, "All transaction amounts should be positive")\n        \n        # Test date ranges\n        future_dates = transactions.filter(\n            col("transaction_date") > current_date()\n        ).count()\n        self.assertEqual(future_dates, 0, "No transaction dates should be in the future")\n        \n        # Test referential integrity\n        transactions_table = self.spark.table("transactions_silver")\n        customers_table = self.spark.table("customers_silver")\n        \n        orphaned_transactions = transactions_table.join(\n            customers_table,\n            transactions_table.customer_id == customers_table.customer_id,\n            "left_anti"\n        ).count()\n        \n        self.assertEqual(orphaned_transactions, 0, "All transactions should have a matching customer")\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\nif __name__ == "__main__":\n    unittest.main()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-testing-data-pipelines",children:"Hands-on Exercise: Testing Data Pipelines"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Write unit tests for key transformation functions"}),"\n",(0,i.jsx)(n.li,{children:"Implement integration tests for a complete pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Create data quality validation tests"}),"\n",(0,i.jsx)(n.li,{children:"Configure automated test execution"}),"\n",(0,i.jsx)(n.li,{children:"Review test results and improve test coverage"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"automating-deployments",children:"Automating Deployments"}),"\n",(0,i.jsx)(n.h4,{id:"cicd-pipeline-setup",children:"CI/CD Pipeline Setup"}),"\n",(0,i.jsx)(n.p,{children:"Setting up a CI/CD pipeline with GitHub Actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/deploy-databricks.yml\nname: Deploy to Databricks\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install pyspark==3.3.0 delta-spark==2.3.0 pytest pytest-cov\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n    - name: Test with pytest\n      run: |\n        pytest tests/ --cov=src/\n\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install flake8 black isort\n    - name: Lint with flake8\n      run: |\n        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics\n    - name: Check formatting with black\n      run: |\n        black --check src/ tests/\n    - name: Check imports with isort\n      run: |\n        isort --check-only --profile black src/ tests/\n\n  deploy:\n    needs: [test, lint]\n    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n    - name: Install Databricks CLI\n      run: |\n        pip install databricks-cli\n    - name: Deploy to Dev\n      env:\n        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n      run: |\n        databricks bundle deploy --target dev\n    - name: Run Smoke Tests\n      env:\n        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n      run: |\n        python smoketests/run_tests.py\n    - name: Deploy to Prod\n      if: success()\n      env:\n        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST_PROD }}\n        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN_PROD }}\n      run: |\n        databricks bundle deploy --target prod\n"})}),"\n",(0,i.jsx)(n.h4,{id:"multi-environment-configuration",children:"Multi-Environment Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Managing configurations for different environments:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# .databricks/bundle.yml\nbundle:\n  name: retail_analytics\n\ntargets:\n  dev:\n    workspace:\n      host: ${DATABRICKS_HOST}\n      token: ${DATABRICKS_TOKEN}\n    variables:\n      environment: dev\n      data_path: /mnt/dev/retail\n      notification_email: dev-team@example.com\n      auto_optimize: true\n      refresh_schedule: "0 */2 * * *"  # Every 2 hours\n      cluster_workers_min: 2\n      cluster_workers_max: 8\n      \n  qa:\n    workspace:\n      host: ${DATABRICKS_HOST_QA}\n      token: ${DATABRICKS_TOKEN_QA}\n    variables:\n      environment: qa\n      data_path: /mnt/qa/retail\n      notification_email: qa-team@example.com\n      auto_optimize: true\n      refresh_schedule: "0 */4 * * *"  # Every 4 hours\n      cluster_workers_min: 2\n      cluster_workers_max: 4\n      \n  prod:\n    workspace:\n      host: ${DATABRICKS_HOST_PROD}\n      token: ${DATABRICKS_TOKEN_PROD}\n    variables:\n      environment: prod\n      data_path: /mnt/prod/retail\n      notification_email: alerts@example.com\n      auto_optimize: true\n      refresh_schedule: "0 */1 * * *"  # Every hour\n      cluster_workers_min: 4\n      cluster_workers_max: 16\n\nresources:\n  pipelines:\n    retail_pipeline:\n      path: resources/pipelines/retail_pipeline.yml\n      target: ${bundle.target}\n      configuration:\n        - key: data_path\n          value: ${variables.data_path}\n        - key: environment\n          value: ${variables.environment}\n          \n  jobs:\n    retail_reports:\n      path: resources/jobs/retail_reports.yml\n      schedule:\n        quartz_cron_expression: ${variables.refresh_schedule}\n        timezone_id: "UTC"\n      email_notifications:\n        on_failure:\n          - ${variables.notification_email}\n      tags:\n        environment: ${variables.environment}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"infrastructure-as-code",children:"Infrastructure as Code"}),"\n",(0,i.jsx)(n.p,{children:"Defining infrastructure with Terraform:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-hcl",children:'# main.tf\nprovider "databricks" {\n  host  = var.databricks_host\n  token = var.databricks_token\n}\n\nresource "databricks_cluster" "processing_cluster" {\n  cluster_name            = "${var.environment}-processing-cluster"\n  spark_version           = "11.3.x-scala2.12"\n  node_type_id            = var.node_type_id\n  autotermination_minutes = 20\n  \n  autoscale {\n    min_workers = var.cluster_workers_min\n    max_workers = var.cluster_workers_max\n  }\n  \n  spark_conf = {\n    "spark.databricks.delta.optimizeWrite.enabled" = "true"\n    "spark.databricks.delta.autoCompact.enabled"   = "true"\n    "spark.sql.adaptive.enabled"                   = "true"\n  }\n  \n  custom_tags = {\n    "Environment" = var.environment\n    "Project"     = "RetailAnalytics"\n  }\n}\n\nresource "databricks_job" "etl_job" {\n  name = "${var.environment}-retail-etl"\n  \n  job_cluster {\n    job_cluster_key = "processing_cluster"\n    \n    new_cluster {\n      spark_version           = "11.3.x-scala2.12"\n      node_type_id            = var.node_type_id\n      autoscale {\n        min_workers = var.cluster_workers_min\n        max_workers = var.cluster_workers_max\n      }\n    }\n  }\n  \n  task {\n    task_key = "ingest"\n    \n    notebook_task {\n      notebook_path = "/Repos/retail_analytics/notebooks/ingest"\n      base_parameters = {\n        "environment" = var.environment\n        "data_path"   = var.data_path\n      }\n    }\n    \n    job_cluster_key = "processing_cluster"\n  }\n  \n  task {\n    task_key = "transform"\n    \n    notebook_task {\n      notebook_path = "/Repos/retail_analytics/notebooks/transform"\n      base_parameters = {\n        "environment" = var.environment\n        "data_path"   = var.data_path\n      }\n    }\n    \n    job_cluster_key = "processing_cluster"\n    \n    depends_on {\n      task_key = "ingest"\n    }\n  }\n  \n  task {\n    task_key = "aggregate"\n    \n    notebook_task {\n      notebook_path = "/Repos/retail_analytics/notebooks/aggregate"\n      base_parameters = {\n        "environment" = var.environment\n        "data_path"   = var.data_path\n      }\n    }\n    \n    job_cluster_key = "processing_cluster"\n    \n    depends_on {\n      task_key = "transform"\n    }\n  }\n  \n  schedule {\n    quartz_cron_expression = var.schedule_cron\n    timezone_id = "UTC"\n  }\n  \n  email_notifications {\n    on_success = []\n    on_failure = [var.notification_email]\n  }\n}\n\n# variables.tf\nvariable "databricks_host" {\n  description = "Databricks workspace URL"\n  type        = string\n}\n\nvariable "databricks_token" {\n  description = "Databricks access token"\n  type        = string\n  sensitive   = true\n}\n\nvariable "environment" {\n  description = "Deployment environment (dev, qa, prod)"\n  type        = string\n}\n\nvariable "node_type_id" {\n  description = "Databricks node type"\n  type        = string\n  default     = "Standard_DS3_v2"\n}\n\nvariable "cluster_workers_min" {\n  description = "Minimum number of workers"\n  type        = number\n}\n\nvariable "cluster_workers_max" {\n  description = "Maximum number of workers"\n  type        = number\n}\n\nvariable "data_path" {\n  description = "Path to data in storage"\n  type        = string\n}\n\nvariable "schedule_cron" {\n  description = "Cron schedule expression"\n  type        = string\n}\n\nvariable "notification_email" {\n  description = "Email for notifications"\n  type        = string\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-setting-up-automated-deployments",children:"Hands-on Exercise: Setting up Automated Deployments"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Configure a GitHub Actions workflow for CI/CD"}),"\n",(0,i.jsx)(n.li,{children:"Create environment-specific configurations"}),"\n",(0,i.jsx)(n.li,{children:"Implement infrastructure as code with Terraform"}),"\n",(0,i.jsx)(n.li,{children:"Set up automated testing and deployment"}),"\n",(0,i.jsx)(n.li,{children:"Practice the complete deployment lifecycle"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4-monitoring-and-observability",children:"4. Monitoring and Observability"}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.h4,{id:"tracking-job-and-query-performance",children:"Tracking Job and Query Performance"}),"\n",(0,i.jsx)(n.p,{children:"Tools and techniques for monitoring performance:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Databricks Job UI"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Job execution history"}),"\n",(0,i.jsx)(n.li,{children:"Task-level runtime metrics"}),"\n",(0,i.jsx)(n.li,{children:"Resource utilization graphs"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Spark UI"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Event timeline"}),"\n",(0,i.jsx)(n.li,{children:"SQL query execution details"}),"\n",(0,i.jsx)(n.li,{children:"Storage tab for caching efficiency"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Custom monitoring using Ganglia metrics"}),":"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Get Ganglia metrics\ndef get_cluster_metrics(cluster_id):\n    response = requests.get(\n        f"https://<databricks-instance>/api/2.0/clusters/get",\n        headers={"Authorization": f"Bearer {token}"},\n        params={"cluster_id": cluster_id}\n    )\n    \n    cluster_info = response.json()\n    spark_ui_url = cluster_info.get("spark_context_id")\n    \n    if spark_ui_url:\n        metrics_url = f"https://<databricks-instance>/driver-proxy-api/o/0/{cluster_id}/{spark_ui_url}/metrics/json/"\n        metrics_response = requests.get(\n            metrics_url,\n            headers={"Authorization": f"Bearer {token}"}\n        )\n        \n        return metrics_response.json()\n    \n    return None\n\n# Track metrics over time\ndef monitor_cluster_performance(cluster_id, interval_seconds=60, duration_minutes=60):\n    metrics_history = []\n    \n    num_samples = int((duration_minutes * 60) / interval_seconds)\n    \n    for i in range(num_samples):\n        metrics = get_cluster_metrics(cluster_id)\n        if metrics:\n            timestamp = datetime.now().isoformat()\n            metrics_history.append({\n                "timestamp": timestamp,\n                "metrics": metrics\n            })\n            \n            # Extract key metrics\n            cpu_usage = metrics.get("gauges", {}).get("system.cpu.usage", {}).get("value")\n            memory_used = metrics.get("gauges", {}).get("system.memory.used", {}).get("value")\n            disk_io = metrics.get("gauges", {}).get("system.disk.io", {}).get("value")\n            \n            print(f"[{timestamp}] CPU: {cpu_usage}%, Memory: {memory_used} bytes, Disk IO: {disk_io} bytes/sec")\n        \n        time.sleep(interval_seconds)\n    \n    return metrics_history\n'})}),"\n",(0,i.jsx)(n.h4,{id:"performance-dashboard",children:"Performance Dashboard"}),"\n",(0,i.jsx)(n.p,{children:"Creating a performance monitoring dashboard:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Create a job performance tracking table\nCREATE TABLE job_performance_metrics (\n  job_id BIGINT,\n  run_id BIGINT,\n  task_key STRING,\n  start_time TIMESTAMP,\n  end_time TIMESTAMP,\n  duration_seconds BIGINT,\n  status STRING,\n  cluster_id STRING,\n  num_workers INT,\n  input_records BIGINT,\n  output_records BIGINT,\n  shuffle_read_bytes BIGINT,\n  shuffle_write_bytes BIGINT,\n  executor_cpu_time_ms BIGINT,\n  executor_peak_memory_usage BIGINT\n);\n\n-- Query for dashboard\nSELECT \n  job_id,\n  task_key,\n  AVG(duration_seconds) as avg_duration,\n  PERCENTILE(duration_seconds, 0.5) as median_duration,\n  PERCENTILE(duration_seconds, 0.95) as p95_duration,\n  MAX(duration_seconds) as max_duration,\n  MIN(duration_seconds) as min_duration,\n  COUNT(*) as execution_count,\n  COUNT(CASE WHEN status = 'SUCCESS' THEN 1 END) / COUNT(*) * 100 as success_rate\nFROM job_performance_metrics\nWHERE start_time > current_date() - INTERVAL 30 DAYS\nGROUP BY job_id, task_key\nORDER BY avg_duration DESC;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"data-quality-monitoring-1",children:"Data Quality Monitoring"}),"\n",(0,i.jsx)(n.h4,{id:"building-data-quality-dashboards",children:"Building Data Quality Dashboards"}),"\n",(0,i.jsx)(n.p,{children:"Implementing a data quality monitoring system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Data quality metrics tracking\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndef calculate_data_quality_metrics(table_name, primary_key=None):\n    """\n    Calculate various data quality metrics for a table\n    """\n    df = spark.table(table_name)\n    column_metrics = []\n    \n    # Get schema information\n    schema = df.schema\n    \n    # Record count\n    row_count = df.count()\n    \n    # Column-level metrics\n    for field in schema.fields:\n        col_name = field.name\n        data_type = str(field.dataType)\n        \n        # Basic metrics for all columns\n        null_count = df.filter(F.col(col_name).isNull()).count()\n        null_percentage = (null_count / row_count) * 100 if row_count > 0 else 0\n        \n        # Type-specific metrics\n        if "string" in data_type.lower():\n            # String-specific metrics\n            empty_count = df.filter(F.col(col_name) == "").count()\n            min_length = df.select(F.min(F.length(F.col(col_name)))).collect()[0][0]\n            max_length = df.select(F.max(F.length(F.col(col_name)))).collect()[0][0]\n            avg_length = df.select(F.avg(F.length(F.col(col_name)))).collect()[0][0]\n            \n            col_metrics = {\n                "column_name": col_name,\n                "data_type": data_type,\n                "null_count": null_count,\n                "null_percentage": null_percentage,\n                "empty_count": empty_count,\n                "min_length": min_length,\n                "max_length": max_length,\n                "avg_length": avg_length\n            }\n        elif any(x in data_type.lower() for x in ["int", "double", "float", "decimal"]):\n            # Numeric-specific metrics\n            min_value = df.select(F.min(F.col(col_name))).collect()[0][0]\n            max_value = df.select(F.max(F.col(col_name))).collect()[0][0]\n            avg_value = df.select(F.avg(F.col(col_name))).collect()[0][0]\n            std_dev = df.select(F.stddev(F.col(col_name))).collect()[0][0]\n            \n            col_metrics = {\n                "column_name": col_name,\n                "data_type": data_type,\n                "null_count": null_count,\n                "null_percentage": null_percentage,\n                "min_value": min_value,\n                "max_value": max_value,\n                "avg_value": avg_value,\n                "std_dev": std_dev\n            }\n        elif "date" in data_type.lower() or "timestamp" in data_type.lower():\n            # Date-specific metrics\n            min_date = df.select(F.min(F.col(col_name))).collect()[0][0]\n            max_date = df.select(F.max(F.col(col_name))).collect()[0][0]\n            \n            col_metrics = {\n                "column_name": col_name,\n                "data_type": data_type,\n                "null_count": null_count,\n                "null_percentage": null_percentage,\n                "min_date": min_date,\n                "max_date": max_date\n            }\n        else:\n            # Default metrics for other types\n            col_metrics = {\n                "column_name": col_name,\n                "data_type": data_type,\n                "null_count": null_count,\n                "null_percentage": null_percentage\n            }\n        \n        column_metrics.append(col_metrics)\n    \n    # Duplicate checks if primary key is provided\n    unique_count = None\n    duplicate_count = None\n    \n    if primary_key:\n        if isinstance(primary_key, list):\n            # Composite key\n            unique_count = df.select(*primary_key).distinct().count()\n        else:\n            # Single column key\n            unique_count = df.select(primary_key).distinct().count()\n        \n        duplicate_count = row_count - unique_count\n    \n    # Create metrics DataFrame\n    metrics_df = spark.createDataFrame(column_metrics)\n    \n    # Add table-level metrics\n    table_metrics = {\n        "table_name": table_name,\n        "row_count": row_count,\n        "column_count": len(schema.fields),\n        "primary_key": primary_key,\n        "unique_count": unique_count,\n        "duplicate_count": duplicate_count,\n        "capture_time": F.current_timestamp()\n    }\n    \n    return metrics_df, table_metrics\n'})}),"\n",(0,i.jsx)(n.h4,{id:"anomaly-detection-for-data-pipelines",children:"Anomaly Detection for Data Pipelines"}),"\n",(0,i.jsx)(n.p,{children:"Implementing automated anomaly detection:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Z-score based anomaly detection for metrics\ndef detect_anomalies(metrics_history_df, metric_column, z_threshold=3.0):\n    """\n    Detect anomalies in a metric using Z-score\n    """\n    # Calculate statistics\n    mean = metrics_history_df.select(F.avg(metric_column)).collect()[0][0]\n    stddev = metrics_history_df.select(F.stddev(metric_column)).collect()[0][0]\n    \n    # Handle zero standard deviation\n    if stddev == 0 or stddev is None:\n        return metrics_history_df.withColumn(\n            "is_anomaly", \n            F.lit(False)\n        ).withColumn(\n            "z_score",\n            F.lit(0)\n        )\n    \n    # Calculate Z-score\n    anomalies_df = metrics_history_df.withColumn(\n        "z_score",\n        F.abs((F.col(metric_column) - mean) / stddev)\n    ).withColumn(\n        "is_anomaly",\n        F.col("z_score") > z_threshold\n    )\n    \n    return anomalies_df\n\n# Moving average based anomaly detection\ndef detect_trend_anomalies(metrics_history_df, metric_column, date_column, window_size=7, threshold_pct=20):\n    """\n    Detect anomalies based on deviation from moving average\n    """\n    # Define window spec for moving average\n    window_spec = Window.orderBy(date_column).rowsBetween(-window_size, -1)\n    \n    # Calculate moving average\n    trend_df = metrics_history_df.withColumn(\n        "moving_avg", \n        F.avg(metric_column).over(window_spec)\n    )\n    \n    # Calculate percent deviation from moving average\n    trend_df = trend_df.withColumn(\n        "pct_deviation",\n        F.when(\n            F.col("moving_avg").isNotNull() & (F.col("moving_avg") != 0),\n            F.abs((F.col(metric_column) - F.col("moving_avg")) / F.col("moving_avg") * 100)\n        ).otherwise(0)\n    )\n    \n    # Flag anomalies\n    trend_df = trend_df.withColumn(\n        "is_trend_anomaly",\n        (F.col("pct_deviation") > threshold_pct) & F.col("moving_avg").isNotNull()\n    )\n    \n    return trend_df\n'})}),"\n",(0,i.jsx)(n.h3,{id:"alerting-and-notification-systems",children:"Alerting and Notification Systems"}),"\n",(0,i.jsx)(n.h4,{id:"setting-up-alerts",children:"Setting Up Alerts"}),"\n",(0,i.jsx)(n.p,{children:"Configuring alerts for various conditions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Define alert thresholds\nalert_config = {\n    "job_failure": {\n        "description": "Job execution failed",\n        "severity": "high",\n        "notification_channels": ["email", "slack"]\n    },\n    "data_quality": {\n        "null_percentage": {\n            "threshold": 5.0,  # Alert if null percentage > 5%\n            "description": "High null percentage detected",\n            "severity": "medium",\n            "notification_channels": ["email"]\n        },\n        "duplicate_percentage": {\n            "threshold": 1.0,  # Alert if duplicate percentage > 1%\n            "description": "Duplicates detected in primary key",\n            "severity": "high",\n            "notification_channels": ["email", "slack"]\n        }\n    },\n    "performance": {\n        "job_duration": {\n            "threshold": 7200,  # Alert if job runs longer than 2 hours\n            "description": "Job execution time exceeded threshold",\n            "severity": "low",\n            "notification_channels": ["email"]\n        },\n        "cluster_memory": {\n            "threshold": 90.0,  # Alert if memory usage > 90%\n            "description": "High cluster memory usage",\n            "severity": "medium",\n            "notification_channels": ["email", "slack"]\n        }\n    }\n}\n\n# Send notification function\ndef send_notification(alert_type, alert_details, alert_config):\n    """\n    Send notification based on alert configuration\n    """\n    config = alert_config.get(alert_type)\n    if not config:\n        for category in alert_config:\n            if isinstance(alert_config[category], dict) and alert_type in alert_config[category]:\n                config = alert_config[category][alert_type]\n                break\n    \n    if not config:\n        print(f"No configuration found for alert type: {alert_type}")\n        return\n    \n    severity = config.get("severity", "medium")\n    description = config.get("description", "Alert triggered")\n    channels = config.get("notification_channels", ["email"])\n    \n    message = f"""\n    ALERT: {description}\n    Severity: {severity.upper()}\n    Details: {alert_details}\n    Time: {datetime.now().isoformat()}\n    """\n    \n    # Send to each configured channel\n    for channel in channels:\n        if channel == "email":\n            send_email_alert(message, severity)\n        elif channel == "slack":\n            send_slack_alert(message, severity)\n        # Add other channels as needed\n'})}),"\n",(0,i.jsx)(n.h4,{id:"integration-with-incident-management",children:"Integration with Incident Management"}),"\n",(0,i.jsx)(n.p,{children:"Setting up integration with PagerDuty or similar services:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# PagerDuty integration\nimport requests\nimport json\n\ndef create_pagerduty_incident(summary, details, severity="warning"):\n    """\n    Create an incident in PagerDuty\n    """\n    url = "https://api.pagerduty.com/incidents"\n    headers = {\n        "Content-Type": "application/json",\n        "Accept": "application/vnd.pagerduty+json;version=2",\n        "Authorization": f"Token token={PAGERDUTY_API_KEY}",\n        "From": PAGERDUTY_EMAIL\n    }\n    \n    payload = {\n        "incident": {\n            "type": "incident",\n            "title": summary,\n            "service": {\n                "id": PAGERDUTY_SERVICE_ID,\n                "type": "service_reference"\n            },\n            "urgency": "high" if severity in ["high", "critical"] else "low",\n            "body": {\n                "type": "incident_body",\n                "details": json.dumps(details, indent=2)\n            }\n        }\n    }\n    \n    response = requests.post(url, headers=headers, json=payload)\n    \n    if response.status_code == 201:\n        print(f"PagerDuty incident created: {response.json()[\'incident\'][\'id\']}")\n        return response.json()[\'incident\'][\'id\']\n    else:\n        print(f"Failed to create PagerDuty incident: {response.status_code} - {response.text}")\n        return None\n\n# Example usage\ndef monitor_job_and_alert(job_id):\n    """\n    Monitor a job and create incident if it fails\n    """\n    job_status = get_job_status(job_id)\n    \n    if job_status == "FAILED":\n        job_details = get_job_details(job_id)\n        \n        incident_details = {\n            "job_id": job_id,\n            "run_id": job_details.get("run_id"),\n            "start_time": job_details.get("start_time"),\n            "end_time": job_details.get("end_time"),\n            "error_message": job_details.get("error_message"),\n            "cluster_id": job_details.get("cluster_id")\n        }\n        \n        create_pagerduty_incident(\n            summary=f"Databricks Job {job_id} Failed",\n            details=incident_details,\n            severity="high"\n        )\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-implementing-monitoring-and-alerting",children:"Hands-on Exercise: Implementing Monitoring and Alerting"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up performance dashboards for pipeline monitoring"}),"\n",(0,i.jsx)(n.li,{children:"Implement data quality tracking with historical trends"}),"\n",(0,i.jsx)(n.li,{children:"Configure alerting for critical pipeline failures"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with incident management systems"}),"\n",(0,i.jsx)(n.li,{children:"Set up automated remediation for common issues"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"5-advanced-optimization-techniques",children:"5. Advanced Optimization Techniques"}),"\n",(0,i.jsx)(n.h3,{id:"query-and-pipeline-optimization-patterns",children:"Query and Pipeline Optimization Patterns"}),"\n",(0,i.jsx)(n.h4,{id:"cost-based-optimization",children:"Cost-Based Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Leveraging Spark's cost-based optimizer:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Collect and analyze table statistics\nANALYZE TABLE sales COMPUTE STATISTICS;\nANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS customer_id, product_id, transaction_date;\n\n-- Join reordering will be done automatically based on statistics\nSELECT \n  c.name,\n  p.product_name,\n  SUM(s.quantity) as total_quantity,\n  SUM(s.amount) as total_amount\nFROM sales s\nJOIN customers c ON s.customer_id = c.id\nJOIN products p ON s.product_id = p.id\nWHERE \n  s.transaction_date >= '2023-01-01'\n  AND c.region = 'North America'\n  AND p.category = 'Electronics'\nGROUP BY \n  c.name,\n  p.product_name;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"dynamic-partition-pruning",children:"Dynamic Partition Pruning"}),"\n",(0,i.jsx)(n.p,{children:"Optimizing queries for partitioned tables:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Create a partitioned table\nCREATE TABLE sales_by_date\nPARTITIONED BY (transaction_date)\nAS SELECT * FROM sales;\n\n-- Query that benefits from partition pruning\nSELECT\n  store_id,\n  SUM(amount) as total_sales\nFROM sales_by_date\nWHERE \n  transaction_date BETWEEN '2023-01-01' AND '2023-01-31'\n  AND amount > 100\nGROUP BY store_id;\n\n-- Controlling partition sizes\nSET spark.sql.files.maxPartitionBytes = 134217728; -- 128 MB\nSET spark.sql.shuffle.partitions = 200;\n\n-- Adaptive query execution\nSET spark.sql.adaptive.enabled = true;\nSET spark.sql.adaptive.coalescePartitions.enabled = true;\nSET spark.sql.adaptive.skewJoin.enabled = true;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"skew-handling",children:"Skew Handling"}),"\n",(0,i.jsx)(n.p,{children:"Dealing with data skew in joins and aggregations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import functions as F\n\n# Identify skewed keys\ndef identify_skewed_keys(df, key_column, threshold_pct=10.0):\n    """\n    Identify keys that have skewed distribution\n    """\n    # Count frequency of each key\n    key_counts = df.groupBy(key_column).count()\n    \n    # Calculate total count\n    total_count = df.count()\n    \n    # Calculate percentage for each key\n    key_distribution = key_counts.withColumn(\n        "percentage", \n        (F.col("count") / total_count) * 100\n    ).orderBy(F.desc("percentage"))\n    \n    # Identify keys above threshold\n    skewed_keys = key_distribution.filter(F.col("percentage") > threshold_pct)\n    \n    return skewed_keys\n\n# Handle skewed join\ndef handle_skewed_join(left_df, right_df, join_key, skewed_keys, num_splits=10):\n    """\n    Handle skewed join by splitting the processing of skewed keys\n    """\n    # Filter out skewed keys from normal processing\n    skewed_key_values = [row[join_key] for row in skewed_keys.collect()]\n    \n    normal_left = left_df.filter(~F.col(join_key).isin(skewed_key_values))\n    normal_right = right_df.filter(~F.col(join_key).isin(skewed_key_values))\n    \n    # Process normal keys with regular join\n    normal_joined = normal_left.join(normal_right, join_key)\n    \n    # Process each skewed key separately with salting\n    skewed_joined = None\n    \n    for skewed_key_value in skewed_key_values:\n        left_skewed = left_df.filter(F.col(join_key) == skewed_key_value)\n        right_skewed = right_df.filter(F.col(join_key) == skewed_key_value)\n        \n        # Add a salt column to spread the skewed key\n        left_salted = left_skewed.withColumn(\n            "salt", \n            F.spark_partition_id() % num_splits\n        )\n        \n        # Replicate the right side for each salt value\n        salted_right_skewed = []\n        for i in range(num_splits):\n            salted_right = right_skewed.withColumn("salt", F.lit(i))\n            salted_right_skewed.append(salted_right)\n        \n        right_salted = reduce(lambda x, y: x.union(y), salted_right_skewed)\n        \n        # Join on both the original key and the salt\n        current_joined = left_salted.join(\n            right_salted, \n            [join_key, "salt"]\n        ).drop("salt")\n        \n        # Union with previous results\n        if skewed_joined is None:\n            skewed_joined = current_joined\n        else:\n            skewed_joined = skewed_joined.union(current_joined)\n    \n    # Combine normal and skewed results\n    final_joined = normal_joined.union(skewed_joined)\n    \n    return final_joined\n\n'})}),"\n",(0,i.jsx)(n.h4,{id:"cache-optimization",children:"Cache Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Strategic use of caching for performance gains:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Analyze tables to see which would benefit from caching\ndef analyze_for_caching(tables, min_size_gb=1, max_size_gb=20):\n    """\n    Analyze tables to determine caching candidates\n    """\n    cache_candidates = []\n    \n    for table_name in tables:\n        # Get table size\n        table_size_bytes = spark.sql(f"DESCRIBE DETAIL {table_name}").select("sizeInBytes").collect()[0][0]\n        table_size_gb = table_size_bytes / (1024 * 1024 * 1024)\n        \n        # Get access frequency (simplified - in production, use query history API)\n        query_count = spark.sql(f"""\n            SELECT COUNT(*) FROM system.query_history \n            WHERE query LIKE \'%{table_name}%\' \n            AND query_start_time > current_timestamp() - INTERVAL 7 DAYS\n        """).collect()[0][0]\n        \n        # Check if table is a good candidate for caching\n        if (table_size_gb >= min_size_gb and \n            table_size_gb <= max_size_gb and \n            query_count > 10):\n            \n            cache_candidates.append({\n                "table_name": table_name,\n                "size_gb": table_size_gb,\n                "query_count": query_count,\n                "priority_score": query_count / table_size_gb  # Higher is better\n            })\n    \n    # Sort by priority score\n    cache_candidates.sort(key=lambda x: x["priority_score"], reverse=True)\n    \n    return cache_candidates\n\n# Cache tables with refresh strategy\ndef cache_with_refresh_strategy(table_name, refresh_interval_minutes=60):\n    """\n    Cache a table with a scheduled refresh\n    """\n    # Cache the table\n    spark.sql(f"CACHE TABLE {table_name}")\n    \n    # Schedule a job to refresh the cache\n    from datetime import datetime, timedelta\n    \n    refresh_job = {\n        "name": f"Refresh Cache - {table_name}",\n        "schedule": {\n            "quartz_cron_expression": f"0 */{refresh_interval_minutes} * * * ?",\n            "timezone_id": "UTC"\n        },\n        "tasks": [\n            {\n                "task_key": "refresh_cache",\n                "sql_task": {\n                    "query": f"REFRESH TABLE {table_name}",\n                    "warehouse_id": "${warehouse_id}"\n                }\n            }\n        ]\n    }\n    \n    # Create job using Jobs API\n    create_job(refresh_job)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-advanced-query-optimization",children:"Hands-on Exercise: Advanced Query Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Analyze query performance using Spark UI"}),"\n",(0,i.jsx)(n.li,{children:"Identify and optimize skewed joins"}),"\n",(0,i.jsx)(n.li,{children:"Implement dynamic partition pruning"}),"\n",(0,i.jsx)(n.li,{children:"Set up strategic caching for frequently accessed tables"}),"\n",(0,i.jsx)(n.li,{children:"Measure and document performance improvements"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"resource-management-and-isolation",children:"Resource Management and Isolation"}),"\n",(0,i.jsx)(n.h4,{id:"multi-tenant-resource-allocation",children:"Multi-Tenant Resource Allocation"}),"\n",(0,i.jsx)(n.p,{children:"Managing resources across teams and workloads:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create isolated workspaces\ndef setup_isolated_workspace(team_name, users, workspace_url, token):\n    """\n    Set up an isolated workspace for a team\n    """\n    import requests\n    import json\n    \n    # Create group for the team\n    group_response = requests.post(\n        f"{workspace_url}/api/2.0/groups/create",\n        headers={"Authorization": f"Bearer {token}"},\n        json={"group_name": f"{team_name}-group"}\n    )\n    \n    group_id = group_response.json().get("group_id")\n    \n    # Add users to the group\n    for user in users:\n        requests.post(\n            f"{workspace_url}/api/2.0/groups/add-member",\n            headers={"Authorization": f"Bearer {token}"},\n            json={\n                "group_id": group_id,\n                "user_name": user\n            }\n        )\n    \n    # Create instance pool for the team\n    pool_response = requests.post(\n        f"{workspace_url}/api/2.0/instance-pools/create",\n        headers={"Authorization": f"Bearer {token}"},\n        json={\n            "instance_pool_name": f"{team_name}-pool",\n            "node_type_id": "Standard_DS3_v2",\n            "min_idle_instances": 2,\n            "max_capacity": 20,\n            "idle_instance_autotermination_minutes": 20,\n            "enable_elastic_disk": True,\n            "preloaded_spark_versions": ["11.3.x-scala2.12"]\n        }\n    )\n    \n    pool_id = pool_response.json().get("instance_pool_id")\n    \n    # Create cluster policy for the team\n    policy_response = requests.post(\n        f"{workspace_url}/api/2.0/policies/clusters/create",\n        headers={"Authorization": f"Bearer {token}"},\n        json={\n            "name": f"{team_name}-policy",\n            "definition": json.dumps({\n                "instance_pool_id": {\n                    "type": "fixed",\n                    "value": pool_id\n                },\n                "spark_version": {\n                    "type": "regex",\n                    "pattern": "11.[0-9].*"\n                },\n                "autoscale.min_workers": {\n                    "type": "range",\n                    "min": 1,\n                    "max": 10\n                },\n                "autoscale.max_workers": {\n                    "type": "range",\n                    "min": 2,\n                    "max": 20\n                },\n                "custom_tags.team": {\n                    "type": "fixed",\n                    "value": team_name\n                }\n            })\n        }\n    )\n    \n    policy_id = policy_response.json().get("policy_id")\n    \n    # Grant permissions to the group for the pool and policy\n    requests.post(\n        f"{workspace_url}/api/2.0/permissions/instance-pools/{pool_id}",\n        headers={"Authorization": f"Bearer {token}"},\n        json={\n            "access_control_list": [\n                {\n                    "group_name": f"{team_name}-group",\n                    "permission_level": "CAN_ATTACH_TO"\n                }\n            ]\n        }\n    )\n    \n    requests.post(\n        f"{workspace_url}/api/2.0/permissions/cluster-policies/{policy_id}",\n        headers={"Authorization": f"Bearer {token}"},\n        json={\n            "access_control_list": [\n                {\n                    "group_name": f"{team_name}-group",\n                    "permission_level": "CAN_USE"\n                }\n            ]\n        }\n    )\n    \n    return {\n        "group_id": group_id,\n        "pool_id": pool_id,\n        "policy_id": policy_id\n    }\n'})}),"\n",(0,i.jsx)(n.h4,{id:"resource-quotas-and-limits",children:"Resource Quotas and Limits"}),"\n",(0,i.jsx)(n.p,{children:"Setting up resource quotas for teams:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Configure workspace-level quotas\ndef set_team_quotas(team_name, max_dbus_per_day, max_concurrent_jobs, workspace_url, token):\n    """\n    Set resource quotas for a team\n    """\n    import requests\n    \n    # Get the group ID for the team\n    group_response = requests.get(\n        f"{workspace_url}/api/2.0/groups/list",\n        headers={"Authorization": f"Bearer {token}"}\n    )\n    \n    groups = group_response.json().get("groups", [])\n    team_group = next((g for g in groups if g["display_name"] == f"{team_name}-group"), None)\n    \n    if not team_group:\n        raise Exception(f"Group for team {team_name} not found")\n    \n    group_id = team_group["id"]\n    \n    # Set job concurrency limit for the group\n    requests.post(\n        f"{workspace_url}/api/2.0/jobs/config",\n        headers={"Authorization": f"Bearer {token}"},\n        json={\n            "group_id": group_id,\n            "max_concurrent_runs": max_concurrent_jobs\n        }\n    )\n    \n    # Set cluster quota policy\n    policy_def = {\n        "cluster_dbu_quota": {\n            "type": "fixed",\n            "value": str(max_dbus_per_day)\n        },\n        "cluster_size_quota": {\n            "type": "fixed",\n            "value": "enabled"\n        }\n    }\n    \n    # Update the cluster policy with quotas\n    policies_response = requests.get(\n        f"{workspace_url}/api/2.0/policies/clusters/list",\n        headers={"Authorization": f"Bearer {token}"}\n    )\n    \n    policies = policies_response.json().get("policies", [])\n    team_policy = next((p for p in policies if p["name"] == f"{team_name}-policy"), None)\n    \n    if team_policy:\n        policy_id = team_policy["policy_id"]\n        \n        # Get current policy definition\n        policy_response = requests.get(\n            f"{workspace_url}/api/2.0/policies/clusters/get?policy_id={policy_id}",\n            headers={"Authorization": f"Bearer {token}"}\n        )\n        \n        current_def = json.loads(policy_response.json().get("definition", "{}"))\n        \n        # Update with quota settings\n        current_def.update(policy_def)\n        \n        # Update the policy\n        requests.post(\n            f"{workspace_url}/api/2.0/policies/clusters/edit",\n            headers={"Authorization": f"Bearer {token}"},\n            json={\n                "policy_id": policy_id,\n                "name": f"{team_name}-policy",\n                "definition": json.dumps(current_def)\n            }\n        )\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-resource-management",children:"Hands-on Exercise: Resource Management"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up isolated workspaces for different teams"}),"\n",(0,i.jsx)(n.li,{children:"Configure instance pools for workload isolation"}),"\n",(0,i.jsx)(n.li,{children:"Implement cluster policies with resource constraints"}),"\n",(0,i.jsx)(n.li,{children:"Set up job concurrency limits and DBU quotas"}),"\n",(0,i.jsx)(n.li,{children:"Monitor resource utilization across teams"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"6-security-and-governance-best-practices",children:"6. Security and Governance Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"advanced-security-configuration",children:"Advanced Security Configuration"}),"\n",(0,i.jsx)(n.h4,{id:"unity-catalog-setup",children:"Unity Catalog Setup"}),"\n",(0,i.jsx)(n.p,{children:"Configuring Unity Catalog for multi-level security:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Create a metastore\nCREATE METASTORE main_metastore\nCOMMENT 'Main metastore for the organization'\nLOCATION 's3://company-unity-catalog/main-metastore';\n\n-- Create catalogs for different domains\nCREATE CATALOG retail\nCOMMENT 'Retail domain data assets';\n\nCREATE CATALOG finance\nCOMMENT 'Financial data and reporting';\n\n-- Create schemas in catalogs\nCREATE SCHEMA retail.raw\nCOMMENT 'Raw/Bronze layer data for retail';\n\nCREATE SCHEMA retail.trusted\nCOMMENT 'Trusted/Silver layer for retail';\n\nCREATE SCHEMA retail.curated\nCOMMENT 'Curated/Gold layer for retail';\n\n-- Grant catalog permissions to groups\nGRANT CREATE, USAGE ON CATALOG retail TO `retail_admins`;\nGRANT USAGE ON CATALOG retail TO `retail_users`;\n\n-- Grant schema permissions\nGRANT CREATE, USAGE ON SCHEMA retail.raw TO `retail_engineers`;\nGRANT USAGE ON SCHEMA retail.trusted TO `retail_analysts`;\nGRANT USAGE ON SCHEMA retail.curated TO `retail_analysts`;\n\n-- Set up external locations\nCREATE EXTERNAL LOCATION retail_landing\nURL 's3://company-data-lake/retail/landing'\nWITH (CREDENTIAL aws_iam_role_retail);\n\nGRANT READ FILES, WRITE FILES ON EXTERNAL LOCATION retail_landing TO `retail_engineers`;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"advanced-access-control",children:"Advanced Access Control"}),"\n",(0,i.jsx)(n.p,{children:"Implementing fine-grained access control:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Column-level security\nCREATE TABLE finance.trusted.employee_data (\n  employee_id STRING,\n  name STRING,\n  email STRING,\n  department STRING,\n  salary DOUBLE,\n  hire_date DATE,\n  manager_id STRING,\n  address STRING,\n  phone_number STRING,\n  tax_id STRING\n);\n\n-- Grant access to specific columns only\nGRANT SELECT ON TABLE finance.trusted.employee_data TO `hr_team`;\nGRANT SELECT (employee_id, name, email, department, hire_date, manager_id) \n  ON TABLE finance.trusted.employee_data TO `managers`;\n\n-- Row-level security with secure views\nCREATE VIEW finance.curated.department_employees\nAS SELECT \n  employee_id,\n  name,\n  email,\n  department,\n  hire_date,\n  manager_id\nFROM finance.trusted.employee_data\nWHERE department IN \n  (SELECT allowed_departments FROM access_control.department_permissions \n   WHERE username = current_user());\n\n-- Dynamic data masking\nCREATE TABLE customers (\n  customer_id STRING,\n  name STRING,\n  email STRING,\n  address STRING,\n  phone STRING,\n  tax_id STRING,\n  birth_date DATE,\n  signup_date DATE\n);\n\n-- Create masked version\nCREATE VIEW masked_customers\nAS SELECT\n  customer_id,\n  CASE \n    WHEN is_member('pii_admins') THEN name\n    ELSE regexp_replace(name, '(^\\\\w)\\\\w+\\\\s(\\\\w)\\\\w+', '$1*** $2***')\n  END AS name,\n  CASE\n    WHEN is_member('pii_admins') THEN email\n    ELSE regexp_replace(email, '(^\\\\w{1,2})\\\\w+(@.+)', '$1***$2')\n  END AS email,\n  CASE\n    WHEN is_member('pii_admins') THEN address\n    ELSE '*** Masked ***'\n  END AS address,\n  CASE\n    WHEN is_member('pii_admins') THEN phone\n    ELSE regexp_replace(phone, '\\\\d(?=\\\\d{4})', '*')\n  END AS phone,\n  CASE\n    WHEN is_member('pii_admins') THEN tax_id\n    ELSE '*** Masked ***'\n  END AS tax_id,\n  CASE\n    WHEN is_member('pii_admins') THEN birth_date\n    ELSE NULL\n  END AS birth_date,\n  signup_date\nFROM customers;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"encryption-and-key-management",children:"Encryption and Key Management"}),"\n",(0,i.jsx)(n.p,{children:"Configuring encryption for sensitive data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Set up encryption for a cluster\ndef create_encrypted_cluster(cluster_name, key_arn):\n    """\n    Create a cluster with encryption configuration\n    """\n    cluster_config = {\n        "cluster_name": cluster_name,\n        "spark_version": "11.3.x-scala2.12",\n        "node_type_id": "Standard_DS3_v2",\n        "autoscale": {\n            "min_workers": 2,\n            "max_workers": 8\n        },\n        "encryption": {\n            "encryption_type": "AWS_KMS",\n            "encryption_at_storage": {\n                "aws_kms": {\n                    "key_arn": key_arn\n                }\n            }\n        },\n        "spark_conf": {\n            "spark.databricks.io.encryption.enabled": "true",\n            "spark.databricks.io.encryption.keySources.aws.enabled": "true"\n        }\n    }\n    \n    response = requests.post(\n        f"{workspace_url}/api/2.0/clusters/create",\n        headers={"Authorization": f"Bearer {token}"},\n        json=cluster_config\n    )\n    \n    return response.json()\n\n# Set up table-level encryption\ndef setup_table_encryption(table_name, catalog, schema, cmk_id):\n    """\n    Configure table-level encryption\n    """\n    # Create encrypted managed table\n    spark.sql(f"""\n    CREATE TABLE {catalog}.{schema}.{table_name} (\n      id STRING,\n      data STRING,\n      sensitive_info STRING\n    )\n    TBLPROPERTIES (\n      \'encryption.algorithm\' = \'AES_GCM_256\',\n      \'encryption.key.id\' = \'{cmk_id}\'\n    )\n    """)\n    \n    return f"Table {catalog}.{schema}.{table_name} created with encryption"\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-security-implementation",children:"Hands-on Exercise: Security Implementation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up Unity Catalog with multi-level organization"}),"\n",(0,i.jsx)(n.li,{children:"Implement column-level security for sensitive data"}),"\n",(0,i.jsx)(n.li,{children:"Create dynamic masking views for PII"}),"\n",(0,i.jsx)(n.li,{children:"Configure encryption for data at rest and in transit"}),"\n",(0,i.jsx)(n.li,{children:"Audit and test security controls"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-governance-and-compliance",children:"Data Governance and Compliance"}),"\n",(0,i.jsx)(n.h4,{id:"data-lineage-tracking",children:"Data Lineage Tracking"}),"\n",(0,i.jsx)(n.p,{children:"Implementing data lineage with Unity Catalog:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Query lineage information\ndef get_table_lineage(catalog_name, schema_name, table_name):\n    """\n    Get upstream and downstream lineage for a table\n    """\n    lineage_df = spark.sql(f"""\n    SELECT *\n    FROM system.information_schema.tables_column_lineage\n    WHERE target_table_catalog = \'{catalog_name}\'\n      AND target_table_schema = \'{schema_name}\'\n      AND target_table_name = \'{table_name}\'\n    ORDER BY target_column_name\n    """)\n    \n    return lineage_df\n\n# Extract lineage from DLT pipelines\ndef extract_dlt_lineage(pipeline_id):\n    """\n    Extract lineage information from a DLT pipeline\n    """\n    import requests\n    \n    # Get pipeline details\n    pipeline_response = requests.get(\n        f"{workspace_url}/api/2.0/pipelines/{pipeline_id}",\n        headers={"Authorization": f"Bearer {token}"}\n    )\n    \n    pipeline = pipeline_response.json()\n    \n    # Get latest update\n    updates_response = requests.get(\n        f"{workspace_url}/api/2.0/pipelines/{pipeline_id}/updates",\n        headers={"Authorization": f"Bearer {token}"}\n    )\n    \n    updates = updates_response.json().get("updates", [])\n    \n    if updates:\n        latest_update = updates[0]\n        update_id = latest_update.get("update_id")\n        \n        # Get lineage from the latest update\n        lineage_response = requests.get(\n            f"{workspace_url}/api/2.0/lineage-tracking/table-lineages",\n            headers={"Authorization": f"Bearer {token}"},\n            params={\n                "pipeline_id": pipeline_id,\n                "update_id": update_id\n            }\n        )\n        \n        return lineage_response.json()\n    \n    return None\n'})}),"\n",(0,i.jsx)(n.h4,{id:"data-cataloging-and-documentation",children:"Data Cataloging and Documentation"}),"\n",(0,i.jsx)(n.p,{children:"Setting up comprehensive data cataloging:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create documentation for a table\ndef document_table(catalog, schema, table_name, description, tags=None, column_descriptions=None):\n    """\n    Add documentation to a table\n    """\n    # Set table description\n    spark.sql(f"""\n    COMMENT ON TABLE {catalog}.{schema}.{table_name} IS \'{description}\'\n    """)\n    \n    # Set table properties for tags\n    if tags:\n        tags_str = ", ".join([f"\'{k}\': \'{v}\'" for k, v in tags.items()])\n        spark.sql(f"""\n        ALTER TABLE {catalog}.{schema}.{table_name} \n        SET TBLPROPERTIES (\'tags\' = \'{{{tags_str}}}\')\n        """)\n    \n    # Set column descriptions\n    if column_descriptions:\n        for column, desc in column_descriptions.items():\n            spark.sql(f"""\n            COMMENT ON COLUMN {catalog}.{schema}.{table_name}.{column} IS \'{desc}\'\n            """)\n    \n    return f"Documentation added for {catalog}.{schema}.{table_name}"\n\n# Generate data dictionary\ndef generate_data_dictionary(catalog, schema=None, table=None):\n    """\n    Generate a data dictionary for the specified scope\n    """\n    if table:\n        # Get table details\n        table_details = spark.sql(f"""\n        DESCRIBE TABLE EXTENDED {catalog}.{schema}.{table}\n        """)\n        \n        # Get column details\n        columns = spark.sql(f"""\n        SELECT \n          column_name,\n          data_type,\n          comment\n        FROM system.information_schema.columns\n        WHERE table_catalog = \'{catalog}\'\n          AND table_schema = \'{schema}\'\n          AND table_name = \'{table}\'\n        ORDER BY ordinal_position\n        """)\n        \n        return {\n            "table_name": f"{catalog}.{schema}.{table}",\n            "details": {row["col_name"]: row["data_type"] for row in table_details.collect() if not row["col_name"].startswith("#")},\n            "comment": spark.sql(f"SELECT comment FROM system.information_schema.tables WHERE table_catalog = \'{catalog}\' AND table_schema = \'{schema}\' AND table_name = \'{table}\'").collect()[0]["comment"],\n            "columns": [{\n                "name": row["column_name"],\n                "type": row["data_type"],\n                "description": row["comment"]\n            } for row in columns.collect()]\n        }\n    elif schema:\n        # Get all tables in the schema\n        tables = spark.sql(f"""\n        SELECT table_name, comment\n        FROM system.information_schema.tables\n        WHERE table_catalog = \'{catalog}\'\n          AND table_schema = \'{schema}\'\n        ORDER BY table_name\n        """)\n        \n        result = {\n            "schema_name": f"{catalog}.{schema}",\n            "tables": [{\n                "name": row["table_name"],\n                "description": row["comment"]\n            } for row in tables.collect()]\n        }\n        \n        return result\n    else:\n        # Get all schemas in the catalog\n        schemas = spark.sql(f"""\n        SELECT schema_name\n        FROM system.information_schema.schemata\n        WHERE catalog_name = \'{catalog}\'\n        ORDER BY schema_name\n        """)\n        \n        result = {\n            "catalog_name": catalog,\n            "schemas": [{\n                "name": row["schema_name"]\n            } for row in schemas.collect()]\n        }\n        \n        return result\n'})}),"\n",(0,i.jsx)(n.h4,{id:"regulatory-compliance",children:"Regulatory Compliance"}),"\n",(0,i.jsx)(n.p,{children:"Implementing controls for regulatory compliance:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# PII detection function\ndef detect_pii(df, threshold=0.7):\n    """\n    Detect columns that may contain PII\n    """\n    from pyspark.sql import functions as F\n    import re\n    \n    # Sample the dataframe\n    sample_df = df.limit(1000)\n    \n    # Define patterns for common PII\n    patterns = {\n        "email": r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\',\n        "phone": r\'(\\+\\d{1,3}[- ]?)?\\(?\\d{3}\\)?[- ]?\\d{3}[- ]?\\d{4}\',\n        "ssn": r\'\\d{3}-\\d{2}-\\d{4}\',\n        "credit_card": r\'\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\',\n        "ip_address": r\'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\',\n        "address": r\'\\d+\\s+[a-zA-Z]+\\s+[a-zA-Z]+\',\n        "name": r\'^[A-Z][a-z]+ [A-Z][a-z]+\n    }\n    \n    results = []\n    \n    # Check each string column\n    for col_name in df.schema.names:\n        col_type = df.schema[col_name].dataType.simpleString()\n        \n        if "string" in col_type.lower():\n            # Check each pattern\n            for pii_type, pattern in patterns.items():\n                # Count matches\n                match_count = sample_df.filter(\n                    F.col(col_name).rlike(pattern)\n                ).count()\n                \n                match_pct = match_count / sample_df.count() if sample_df.count() > 0 else 0\n                \n                if match_pct >= threshold:\n                    results.append({\n                        "column_name": col_name,\n                        "pii_type": pii_type,\n                        "confidence": match_pct\n                    })\n    \n    return results\n\n# Anonymization function\ndef anonymize_pii(df, pii_columns):\n    """\n    Anonymize PII in a dataframe\n    """\n    from pyspark.sql import functions as F\n    \n    result_df = df\n    \n    # Apply appropriate anonymization technique for each column\n    for col_info in pii_columns:\n        col_name = col_info["column_name"]\n        pii_type = col_info["pii_type"]\n        \n        if pii_type == "email":\n            result_df = result_df.withColumn(\n                col_name,\n                F.regexp_replace(F.col(col_name), r\'([^@\\s]+)@\', r\'****@\')\n            )\n        elif pii_type in ["phone", "ssn", "credit_card"]:\n            result_df = result_df.withColumn(\n                col_name,\n                F.regexp_replace(F.col(col_name), r\'\\d(?=\\d{4})\', \'*\')\n            )\n        elif pii_type == "name":\n            result_df = result_df.withColumn(\n                col_name,\n                F.concat(\n                    F.substring(F.col(col_name), 1, 1),\n                    F.lit("*** "),\n                    F.substring_index(F.col(col_name), " ", -1)\n                )\n            )\n        elif pii_type == "address":\n            result_df = result_df.withColumn(\n                col_name,\n                F.lit("**** [REDACTED ADDRESS] ****")\n            )\n    \n    return result_df\n'})}),"\n",(0,i.jsx)(n.h4,{id:"hands-on-exercise-data-governance-implementation",children:"Hands-on Exercise: Data Governance Implementation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up data lineage tracking"}),"\n",(0,i.jsx)(n.li,{children:"Implement comprehensive data cataloging"}),"\n",(0,i.jsx)(n.li,{children:"Configure PII detection and protection"}),"\n",(0,i.jsx)(n.li,{children:"Set up audit logging and compliance reporting"}),"\n",(0,i.jsx)(n.li,{children:"Create governance dashboards"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary-and-key-takeaways",children:"Summary and Key Takeaways"}),"\n",(0,i.jsx)(n.p,{children:"This advanced training has covered critical aspects of optimizing and productionizing data engineering workflows on Databricks:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Performance Optimization"}),": Techniques for improving query and pipeline performance, including physical layout optimization, Liquid Clustering, and handling data skew."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Advanced Pipeline Patterns"}),": Implementation of complex patterns like SCD Type 2, CDC processing, and incremental aggregation for robust data pipelines."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"CI/CD for Data Engineering"}),": Setting up complete CI/CD pipelines with testing, automated deployment, and infrastructure as code."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Monitoring and Observability"}),": Building comprehensive monitoring systems for performance, data quality, and incident management."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Security and Governance"}),": Implementing Unity Catalog, fine-grained access control, and data governance practices for enterprise-grade data management."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"By applying these advanced techniques, data engineers can build high-performance, resilient, and secure data pipelines that deliver reliable results at scale."}),"\n",(0,i.jsx)(n.h3,{id:"recommended-next-steps",children:"Recommended Next Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Apply these techniques incrementally to your existing pipelines"}),"\n",(0,i.jsx)(n.li,{children:"Build a comprehensive CI/CD workflow for your data projects"}),"\n",(0,i.jsx)(n.li,{children:"Implement monitoring and governance frameworks"}),"\n",(0,i.jsx)(n.li,{children:"Pursue advanced Databricks certifications"}),"\n",(0,i.jsx)(n.li,{children:"Stay updated with new Databricks features and best practices"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);